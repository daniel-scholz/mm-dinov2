{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import IPython \n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray\n",
    "from dinov2.data.transforms import make_xray_classification_eval_transform, make_classification_eval_transform\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import ModelWithNormalize, evaluate, extract_features\n",
    "from dinov2.MLkNN import MLkNN \n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230803 13:36:17 3210 dinov2 config.py:60] git:\n",
      "  sha: d9d3e00a8c109a3671cfa08caa07d9c98fa22b30, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20230803 13:36:17 3210 dinov2 config.py:61] batch_size: 8\n",
      "comment: \n",
      "config_file: dinov2/configs/eval/vits14_pretrain.yaml\n",
      "exclude: \n",
      "gather_on_cpu: False\n",
      "n_per_class_list: [-1]\n",
      "n_tries: 1\n",
      "nb_knn: [5, 20, 50, 100, 200]\n",
      "ngpus: 1\n",
      "nodes: 1\n",
      "opts: ['train.output_dir=/mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn']\n",
      "output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "partition: learnlab\n",
      "pretrained_weights: models/dinov2_vits14_pretrain.pth\n",
      "temperature: 0.07\n",
      "timeout: 2800\n",
      "train_dataset_str: NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH/train_tmp\n",
      "use_volta32: False\n",
      "val_dataset_str: NIHChestXray:split=VAL:root=/mnt/d/data/NIH/test_tmp\n",
      "I20230803 13:36:17 3210 dinov2 config.py:27] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20230803 13:36:17 3210 dinov2 config.py:34] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: ImageNet:split=TRAIN\n",
      "  output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_small\n",
      "  patch_size: 14\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 518\n",
      "  local_crops_size: 98\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "I20230803 13:36:17 3210 dinov2 vision_transformer.py:110] using MLP layer as FFN\n",
      "I20230803 13:36:17 3210 dinov2 utils.py:35] Pretrained weights found at models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH/train_tmp', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH/test_tmp', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230803 13:36:19 3210 dinov2 loaders.py:89] using dataset: \"NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH/train_tmp\"\n",
      "I20230803 13:36:21 3210 dinov2 nih_chest_xray.py:67] 86480 x-ray's are missing from TRAIN set\n",
      "I20230803 13:36:21 3210 dinov2 loaders.py:94] # of dataset samples: 44\n",
      "I20230803 13:36:21 3210 dinov2 loaders.py:89] using dataset: \"NIHChestXray:split=VAL:root=/mnt/d/data/NIH/test_tmp\"\n",
      "I20230803 13:36:24 3210 dinov2 nih_chest_xray.py:67] 86480 x-ray's are missing from VAL set\n",
      "I20230803 13:36:24 3210 dinov2 loaders.py:94] # of dataset samples: 44\n"
     ]
    }
   ],
   "source": [
    "transform = make_classification_eval_transform()\n",
    "train_dataset = make_dataset(\n",
    "    dataset_str=args.train_dataset_str,\n",
    "    transform=transform,\n",
    ")\n",
    "val_dataset = make_dataset(\n",
    "    dataset_str=args.val_dataset_str,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_str = args.train_dataset_str\n",
    "val_dataset_str = args.val_dataset_str\n",
    "batch_size = args.batch_size\n",
    "gather_on_cpu = args.gather_on_cpu\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230803 14:17:57 3210 dinov2 loaders.py:164] sampler: none\n",
      "I20230803 14:17:57 3210 dinov2 loaders.py:211] using PyTorch data loader\n",
      "I20230803 14:17:57 3210 dinov2 loaders.py:224] # of batches: 6\n",
      "I20230803 14:17:57 3210 dinov2 utils.py:131] Storing features into tensor of shape torch.Size([44, 384])\n",
      "I20230803 14:17:57 3210 dinov2 helpers.py:103]   [0/6]  eta: 0:00:01    time: 0.262892  data: 0.256785  max mem: 162\n",
      "I20230803 14:17:58 3210 dinov2 helpers.py:103]   [5/6]  eta: 0:00:00    time: 0.218041  data: 0.191460  max mem: 162\n",
      "I20230803 14:17:58 3210 dinov2 helpers.py:131]  Total time: 0:00:01 (0.218460 s / it)\n",
      "I20230803 14:17:58 3210 dinov2 utils.py:143] Features shape: (44, 384)\n",
      "I20230803 14:17:58 3210 dinov2 utils.py:144] Labels shape: (44, 10)\n",
      "I20230803 14:17:58 3210 dinov2 loaders.py:164] sampler: none\n",
      "I20230803 14:17:58 3210 dinov2 loaders.py:211] using PyTorch data loader\n",
      "I20230803 14:17:58 3210 dinov2 loaders.py:224] # of batches: 6\n",
      "I20230803 14:17:59 3210 dinov2 utils.py:131] Storing features into tensor of shape torch.Size([44, 384])\n",
      "I20230803 14:17:59 3210 dinov2 helpers.py:103]   [0/6]  eta: 0:00:01    time: 0.258285  data: 0.250166  max mem: 162\n",
      "I20230803 14:18:00 3210 dinov2 helpers.py:103]   [5/6]  eta: 0:00:00    time: 0.216250  data: 0.195506  max mem: 162\n",
      "I20230803 14:18:00 3210 dinov2 helpers.py:131]  Total time: 0:00:01 (0.216603 s / it)\n",
      "I20230803 14:18:00 3210 dinov2 utils.py:143] Features shape: (44, 384)\n",
      "I20230803 14:18:00 3210 dinov2 utils.py:144] Labels shape: (44, 12)\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast(dtype=autocast_dtype):\n",
    "    train_features, train_labels = extract_features(\n",
    "        model, train_dataset, batch_size, num_workers, gather_on_cpu=gather_on_cpu\n",
    "    )\n",
    "    val_features, val_labels = extract_features(\n",
    "        model, val_dataset, batch_size, num_workers, gather_on_cpu=gather_on_cpu\n",
    "    )\n",
    "\n",
    "train_features = train_features.cpu().numpy()\n",
    "train_labels = train_labels.cpu().numpy()\n",
    "val_features = val_features.cpu().numpy()\n",
    "val_labels = val_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### for 5 NN ###\n",
      "### Hamming loss ###\n",
      "SKML: 0.093182\n",
      "### Accuracy score ###\n",
      "SKML: 0.409091\n",
      "### F1 score ###\n",
      "SKML: 0.481013\n",
      "### for 20 NN ###\n",
      "### Hamming loss ###\n",
      "SKML: 0.104545\n",
      "### Accuracy score ###\n",
      "SKML: 0.363636\n",
      "### F1 score ###\n",
      "SKML: 0.410256\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 44, n_neighbors = 50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m args\u001b[39m.\u001b[39mnb_knn:\n\u001b[1;32m      5\u001b[0m     classifier \u001b[39m=\u001b[39m MLkNN(i)\n\u001b[0;32m----> 6\u001b[0m     classifier\u001b[39m.\u001b[39;49mfit(train_features, train_labels)\n\u001b[1;32m      7\u001b[0m     results \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(train_features)\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m### for \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m NN ###\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/MLkNN.py:238\u001b[0m, in \u001b[0;36mMLkNN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prior_prob_true, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prior_prob_false \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_prior(\n\u001b[1;32m    235\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_cache\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    237\u001b[0m \u001b[39m# Computing the posterior probabilities\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cond_prob_true, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cond_prob_false \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_cond(\n\u001b[1;32m    239\u001b[0m     X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_label_cache\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/MLkNN.py:181\u001b[0m, in \u001b[0;36mMLkNN._compute_cond\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    175\u001b[0m cn \u001b[39m=\u001b[39m sparse\u001b[39m.\u001b[39mlil_matrix((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m label_info \u001b[39m=\u001b[39m get_matrix_in_format(y, \u001b[39m\"\u001b[39m\u001b[39mdok\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    179\u001b[0m neighbors \u001b[39m=\u001b[39m [\n\u001b[1;32m    180\u001b[0m     a[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_first_neighbours :]\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mknn_\u001b[39m.\u001b[39;49mkneighbors(\n\u001b[1;32m    182\u001b[0m         X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_first_neighbours, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m ]\n\u001b[1;32m    186\u001b[0m \u001b[39mfor\u001b[39;00m instance \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_instances):\n\u001b[1;32m    187\u001b[0m     deltas \u001b[39m=\u001b[39m label_info[neighbors[instance], :]\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/neighbors/_base.py:808\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    806\u001b[0m n_samples_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_fit_\n\u001b[1;32m    807\u001b[0m \u001b[39mif\u001b[39;00m n_neighbors \u001b[39m>\u001b[39m n_samples_fit:\n\u001b[0;32m--> 808\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    809\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected n_neighbors <= n_samples, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but n_samples = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, n_neighbors = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[1;32m    811\u001b[0m     )\n\u001b[1;32m    813\u001b[0m n_jobs \u001b[39m=\u001b[39m effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n\u001b[1;32m    814\u001b[0m chunked_results \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 44, n_neighbors = 50"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "for i in args.nb_knn:\n",
    "    classifier = MLkNN(i)\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    results = classifier.predict(train_features).toarray()\n",
    "    print(f\"### for {i} NN ###\")\n",
    "    print(\"### Hamming loss ###\")\n",
    "    print(\"SKML: %f\" % sklearn.metrics.hamming_loss(train_labels, results))\n",
    "    print(\"### Accuracy score ###\")\n",
    "    print(\"SKML: %f\" % sklearn.metrics.accuracy_score(train_labels, results))\n",
    "    print(\"### F1 score ###\")\n",
    "    print(\"SKML: %f\" % sklearn.metrics.f1_score(train_labels, results, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
