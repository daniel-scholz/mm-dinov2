{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT, BTCV\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_zero_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20231008 18:19:02 623 dinov2 config.py:60] git:\n",
      "  sha: c84b9711af7d5c040597e989149bbd2f4d43eab6, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20231008 18:19:02 623 dinov2 config.py:61] batch_size: 8\n",
      "comment: \n",
      "config_file: dinov2/configs/eval/vits14_pretrain.yaml\n",
      "exclude: \n",
      "gather_on_cpu: False\n",
      "n_per_class_list: [-1]\n",
      "n_tries: 1\n",
      "nb_knn: [5, 20, 50, 100, 200]\n",
      "ngpus: 1\n",
      "nodes: 1\n",
      "opts: ['train.output_dir=/mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn']\n",
      "output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "partition: learnlab\n",
      "pretrained_weights: models/dinov2_vits14_pretrain.pth\n",
      "temperature: 0.07\n",
      "test_dataset_str: MC:split=TEST:root=/mnt/z/data/MC\n",
      "timeout: 2800\n",
      "train_dataset_str: MC:split=TRAIN:root=/mnt/z/data/MC\n",
      "use_volta32: False\n",
      "val_dataset_str: MC:split=VAL:root=/mnt/z/data/MC\n",
      "I20231008 18:19:02 623 dinov2 config.py:27] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20231008 18:19:02 623 dinov2 config.py:34] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: ImageNet:split=TRAIN\n",
      "  output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_small\n",
      "  patch_size: 14\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 518\n",
      "  local_crops_size: 98\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "I20231008 18:19:02 623 dinov2 vision_transformer.py:110] using MLP layer as FFN\n",
      "I20231008 18:19:03 623 dinov2 utils.py:35] Pretrained weights found at models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/mnt/d/data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='MC:split=TRAIN:root=/mnt/z/data/MC', val_dataset_str='MC:split=VAL:root=/mnt/z/data/MC', test_dataset_str='MC:split=TEST:root=/mnt/z/data/MC', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='BTCV:split=TRAIN:root=/mnt/z/data/BTCV', val_dataset_str='BTCV:split=VAL:root=/mnt/z/data/BTCV', test_dataset_str='BTCV:split=TEST:root=/mnt/z/data/BTCV', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 4, autocast_ctx, is_3d=False)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Enum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_Split\u001b[39;00m(Enum):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     TRAIN \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     VAL \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Enum' is not defined"
     ]
    }
   ],
   "source": [
    "class _Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        split_lengths = {\n",
    "            _Split.TRAIN: 15,\n",
    "            _Split.VAL: 15,\n",
    "            _Split.TEST: 20,\n",
    "        }\n",
    "        return split_lengths[self]\n",
    "\n",
    "\n",
    "class BTCV(MedicalVisionDataset):\n",
    "    Split = _Split\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        split: \"BTCV.Split\",\n",
    "        root: str,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(split, root, transforms, transform, target_transform)\n",
    "\n",
    "        self._image_path = self._split_dir + os.sep + \"img\"\n",
    "        self.images = np.array(os.listdir(self._image_path))\n",
    "\n",
    "        self.labels = None\n",
    "        if self._split != _Split.TEST:\n",
    "            self._labels_path = self._split_dir + os.sep + \"label\"\n",
    "            self.labels = np.array(os.listdir(self._labels_path))\n",
    "    \n",
    "        self.class_id_mapping = pd.DataFrame([i for i in range(14)],\n",
    "                                    index=[\"background\", \"spleen\", \"rkid\", \"lkid\", \"gall\", \"eso\",\n",
    "                                           \"liver\", \"sto\", \"aorta\", \"IVC\", \"veins\", \"pancreas\",\n",
    "                                           \"rad\", \"lad\"],\n",
    "                                    columns=[\"class_id\"])\n",
    "        self.class_names = np.array(self.class_id_mapping.index)\n",
    "\n",
    "    def _check_size(self):\n",
    "        num_of_images = len(os.listdir(self._split_dir + os.sep + \"img\"))\n",
    "        logging.info(f\"{self._split.length - num_of_images} scans are missing from {self._split.value.upper()} set\")\n",
    "\n",
    "    def get_num_classes(self) -> int:\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def is_3d(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def get_image_data(self, index: int, return_affine_matrix=False) -> np.ndarray:\n",
    "        image_folder_path = self._image_path + os.sep + self.images[index]\n",
    "        image_path = image_folder_path + os.sep + os.listdir(image_folder_path)[0]  \n",
    "\n",
    "        if self._split == _Split.TRAIN:\n",
    "            nifti_image = nib.load(image_path, mmap=False)\n",
    "            proxy = nifti_image.dataobj\n",
    "            slice_indices = proxy.shape[-1] - 1\n",
    "            start = np.random.randint(0, slice_indices-10)\n",
    "            indices = list(range(start, start+10))\n",
    "            image = np.array([proxy[..., i] for i in indices])\n",
    "        else:\n",
    "            nifti_image = nib.load(image_path)\n",
    "            image = nifti_image.get_fdata()\n",
    "            image = image.transpose(2, 0, 1)\n",
    "\n",
    "        image = np.stack((image,)*3, axis=0)\n",
    "        image = torch.from_numpy(image).permute(1, 0, 2, 3).float()\n",
    "\n",
    "        if return_affine_matrix:\n",
    "            affine = nifti_image.affine\n",
    "            return image, affine\n",
    "        \n",
    "        # pre-preprocess\n",
    "        image = torch.clamp(image, max=1024)\n",
    "        return image\n",
    "    \n",
    "    def get_target(self, index: int) -> Tuple[np.ndarray, torch.Tensor, None]:\n",
    "        if self.split == _Split.TEST:\n",
    "            return None\n",
    "\n",
    "        label_folder_path = self._labels_path + os.sep + self.labels[index]\n",
    "        label_path = label_folder_path + os.sep + os.listdir(label_folder_path)[0]  \n",
    "\n",
    "        if self._split == _Split.TRAIN:\n",
    "            nifti_image = nib.load(label_path, mmap=False)\n",
    "            proxy = nifti_image.dataobj\n",
    "            slice_indices = proxy.shape[-1] - 1\n",
    "            start = np.random.randint(0, slice_indices-10)\n",
    "            indices = list(range(start, start+10))\n",
    "            target = np.array([proxy[..., i] for i in indices])\n",
    "        else:\n",
    "            target = nib.load(label_path).get_fdata()\n",
    "            target = target.transpose(2, 0, 1)\n",
    "        \n",
    "        target = torch.from_numpy(target).unsqueeze(0).long()\n",
    "        target = target.permute(1, 0, 2, 3)\n",
    "    \n",
    "        return target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image = self.get_image_data(index)\n",
    "        target = self.get_target(index)\n",
    "\n",
    "        seed = np.random.randint(2147483647) # make a seed with numpy generator \n",
    "        if self.transform is not None:\n",
    "            transformed_image = []\n",
    "            for i in range(len(image)):\n",
    "                np.random.seed(seed), torch.manual_seed(seed) \n",
    "                transformed_image.append(self.transform(image[i]))\n",
    "            image = torch.stack(transformed_image, dim=0).squeeze()\n",
    "\n",
    "        if self.target_transform is not None and target is not None:\n",
    "            transformed_target = []\n",
    "            for i in range(len(target)):\n",
    "                np.random.seed(seed), torch.manual_seed(seed) \n",
    "                transformed_target.append(self.target_transform(target[i]))\n",
    "            target = torch.stack(transformed_target, dim=0).squeeze()\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20231008 18:20:06 623 root 2248798430.py:47] 0 scans are missing from TRAIN set\n"
     ]
    }
   ],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "images = BTCV(root=\"/mnt/z/data/BTCV/\", split=_Split.TRAIN, transform=eval_image_transform, target_transform=eval_target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([10, 3, 224, 224])\n",
      "[0 1 5 6 7 8 9]\n",
      "tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAADFUlEQVR4Ae3ZXXaCMBCGYdrj6Wpcg2tiKayJNXQ1XjUgSkmBjEwmE+3LRSsYQ75nQvCnadgQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBA4N0EPhwDnW/n/jYdwqdp77udT/ma+//dxoef9KvgIpddFSsJOBbIJGVNAW/TMHNMt4CLGbq8wrJGrDFgiJsvo1fAnQLmnamOt4nlvIz3zkmC+BXr+9UGbJo8ESsOGCKuF+Wpo1UHfCrJRmOngDlqs5EoOuwUMBqF4S4BDXGLdO1UwXzvVFJKTgFTw8r3PAHzWfr0RAWN3EWrjKhRYoBUMAF0+Okc1ZGcnApKlGpuQwVrro5kbFRQomTVJsfHYq+vDYOJaPjqu0ntU1T91VrtAYWF3r5I6g8om8qbCZ0C9r389wfdZeizyIR4TXMR1kYX0KmCQ8A+429IQ3/rm0vAsYCFEroEXLe2OUpAG9dbr9NUtTxF41vBXrdCSmR8A0pGKLyZbHX1AgG3hi477h3QfI56B5SVQdHKJeBFMeBnX+oSMLwNLbb5BCyY8GRP2Q2naIc/Hpv5x6VuStXG6frhgOgjk2qltQ7YPXK1j0eLB4KvnlQBja/BbhHm2I4qX8H3ojmyHhAyrmB6RLr6pPs/pZvka3EduvrK15+kp/IVvI4x57EZl7BcwHbOtHxkm7BcwGWqYnul7oPtkGianPFVmLgV6ipsHTDE6uY3auurzKsHHGp3366ri+h+Ql0FT/dTl/kfz077s779IkNA+0lke4YqKqhbRvaBqgi4P0Tds28fsMCNXlSB6FYYJu39iHL+1hLwkWfkmEKNGd8l4K+Ec6SQcN4RzYQ/jaqpYBjZbVJqE0URa1pkxmiZ80Vx2UUAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEDgPwr8AMImOpdRh6sYAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "for i, t in images:\n",
    "    print(i.shape)\n",
    "    print(np.unique(t))\n",
    "    print(t.max())\n",
    "    show_image_from_tensor(t[6].unsqueeze(0) * 50 )\n",
    "    print(images.get_num_classes())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20231008 18:03:39 28123 dinov2 loaders.py:96] using dataset: \"MC:split=TRAIN:root=/mnt/z/data/MC\"\n",
      "I20231008 18:03:39 28123 dinov2 medical_dataset.py:37] 0 scans are missing from TRAIN set\n",
      "I20231008 18:03:39 28123 dinov2 loaders.py:101] # of dataset samples: 69\n",
      "I20231008 18:03:39 28123 dinov2 loaders.py:96] using dataset: \"MC:split=VAL:root=/mnt/z/data/MC\"\n",
      "I20231008 18:03:39 28123 dinov2 medical_dataset.py:37] 0 scans are missing from VAL set\n",
      "I20231008 18:03:39 28123 dinov2 loaders.py:101] # of dataset samples: 23\n",
      "I20231008 18:03:39 28123 dinov2 utils.py:349] Train and val datasets have been combined.\n",
      "I20231008 18:03:39 28123 dinov2 loaders.py:96] using dataset: \"MC:split=TEST:root=/mnt/z/data/MC\"\n",
      "I20231008 18:03:39 28123 dinov2 medical_dataset.py:37] 0 scans are missing from TEST set\n",
      "I20231008 18:03:39 28123 dinov2 loaders.py:101] # of dataset samples: 46\n",
      "I20231008 18:03:39 28123 dinov2 loaders.py:124] sampler: infinite\n",
      "I20231008 18:03:39 28123 dinov2 loaders.py:218] using PyTorch data loader\n",
      "I20231008 18:03:39 28123 dinov2 loaders.py:233] infinite data loader\n"
     ]
    }
   ],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "# train_image_transform = make_classification_train_transform()\n",
    "# eval_image_transform = make_classification_eval_transform()\n",
    "\n",
    "# train_target_transform = eval_target_transform = None\n",
    "\n",
    "# val_dataset_str = args.val_dataset_str\n",
    "val_dataset_str = None\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=args.train_dataset_str, val_dataset_str=val_dataset_str,\n",
    "                                                        test_dataset_str=args.test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "\n",
    "# sampler_type = SamplerType.INFINITE\n",
    "sampler_type = None\n",
    "\n",
    "is_3d = test_dataset.is_3d()\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=train_dataset,\n",
    "#     batch_size=2,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "train_data_loader = make_data_loader(\n",
    "    dataset=images,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=sampler_type,\n",
    "    sampler_advance=0,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=collate_fn_3d if is_3d else None\n",
    ")\n",
    "\n",
    "# val_data_loader = make_data_loader(\n",
    "#     dataset=val_dataset,\n",
    "#     batch_size=1,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n",
      "[0 1 2]\n",
      "torch.Size([224, 224])\n",
      "torch.int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m train_dataset:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39munique(t))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(t\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:243\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 243\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/data/datasets/mc.py:102\u001b[0m, in \u001b[0;36mMC.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m):\n\u001b[1;32m    101\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_image_data(index)\n\u001b[0;32m--> 102\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_target(index)\n\u001b[1;32m    104\u001b[0m     seed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m2147483647\u001b[39m) \u001b[39m# make a seed with numpy generator \u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/data/datasets/mc.py:93\u001b[0m, in \u001b[0;36mMC.get_target\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     89\u001b[0m right_mask[right_mask\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_id_mapping\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39mright_lung\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mclass_id\u001b[39m\u001b[39m\"\u001b[39m]  \n\u001b[1;32m     91\u001b[0m target \u001b[39m=\u001b[39m left_mask \u001b[39m+\u001b[39m right_mask\n\u001b[0;32m---> 93\u001b[0m target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(target)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m target\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, t in train_dataset:\n",
    "    print(np.unique(t))\n",
    "    print(t.shape)\n",
    "    print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoderUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, embed_dim=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*2, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.skip_conv = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upconv(x1)\n",
    "        x2 = self.skip_conv(x2)\n",
    "        scale_factor = (x1.size()[2] / x2.size()[2])\n",
    "        x2 = nn.Upsample(scale_factor=scale_factor, mode=\"bilinear\", align_corners=True)(x2)\n",
    "        x = torch.concat([x1, x2], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, image_size=224):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.embed_dim = in_channels\n",
    "        self.image_size = image_size\n",
    "        self.out_channels = out_channels\n",
    "        self.up1 = UNetDecoderUpBlock(in_channels=in_channels, out_channels=in_channels//2, embed_dim=embed_dim)\n",
    "        self.up2 = UNetDecoderUpBlock(in_channels=in_channels//2, out_channels=in_channels//4, embed_dim=embed_dim)\n",
    "        self.up3 = UNetDecoderUpBlock(in_channels=in_channels//4, out_channels=in_channels//8, embed_dim=embed_dim)\n",
    "        self.up4 = UNetDecoderUpBlock(in_channels=in_channels//8, out_channels=out_channels, embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = w = self.image_size//14\n",
    "\n",
    "        skip1 = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip2 = x[2].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip3 = x[1].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip4 = x[0].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        x1    = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        \n",
    "        x2 = self.up1(x1, skip1)\n",
    "        x3 = self.up2(x2, skip2)\n",
    "        x4 = self.up3(x3, skip3)\n",
    "        x5 = self.up4(x4, skip4)\n",
    "\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m train_data_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_image_data(index)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_target(index)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m     seed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m2147483647\u001b[39m) \u001b[39m# make a seed with numpy generator \u001b[39;00m\n",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     start \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, slice_indices\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(start, start\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([proxy[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     nifti_image \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39mload(image_path)\n",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     start \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, slice_indices\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(start, start\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m     image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([proxy[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     nifti_image \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39mload(image_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/arrayproxy.py:445\u001b[0m, in \u001b[0;36mArrayProxy.__getitem__\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, slicer):\n\u001b[0;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_scaled(dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, slicer\u001b[39m=\u001b[39;49mslicer)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/arrayproxy.py:406\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    404\u001b[0m     scl_inter \u001b[39m=\u001b[39m scl_inter\u001b[39m.\u001b[39mastype(use_dtype)\n\u001b[1;32m    405\u001b[0m \u001b[39m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m scaled \u001b[39m=\u001b[39m apply_read_scaling(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_unscaled(slicer\u001b[39m=\u001b[39;49mslicer), scl_slope, scl_inter)\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     scaled \u001b[39m=\u001b[39m scaled\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mpromote_types(scaled\u001b[39m.\u001b[39mdtype, dtype), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/arrayproxy.py:384\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_fileobj() \u001b[39mas\u001b[39;00m fileobj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    376\u001b[0m         \u001b[39mreturn\u001b[39;00m array_from_file(\n\u001b[1;32m    377\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape,\n\u001b[1;32m    378\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m             mmap\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mmap,\n\u001b[1;32m    383\u001b[0m         )\n\u001b[0;32m--> 384\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_fileobj() \u001b[39mas\u001b[39;00m fileobj:\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m fileslice(\n\u001b[1;32m    386\u001b[0m         fileobj,\n\u001b[1;32m    387\u001b[0m         slicer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m         lock\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock,\n\u001b[1;32m    393\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    143\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/arrayproxy.py:368\u001b[0m, in \u001b[0;36mArrayProxy._get_fileobj\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_opener\n\u001b[1;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mwith\u001b[39;00m openers\u001b[39m.\u001b[39mImageOpener(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_like, keep_open\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m opener:\n\u001b[1;32m    369\u001b[0m         \u001b[39myield\u001b[39;00m opener\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/openers.py:260\u001b[0m, in \u001b[0;36mOpener.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\n\u001b[1;32m    255\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    256\u001b[0m     exc_type: \u001b[39mtype\u001b[39m[\u001b[39mBaseException\u001b[39;00m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    257\u001b[0m     exc_val: \u001b[39mBaseException\u001b[39;00m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     exc_tb: TracebackType \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclose_if_mine()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/openers.py:249\u001b[0m, in \u001b[0;36mOpener.close_if_mine\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Close ``self.fobj`` iff we opened it in the constructor\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mme_opened:\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/openers.py:241\u001b[0m, in \u001b[0;36mOpener.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclose\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m/\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfobj\u001b[39m.\u001b[39;49mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, t in train_data_loader:\n",
    "    print(i.shape)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0026.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 153)\n",
      "/mnt/z/data/BTCV//train/img/img0030.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0010.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0028.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 100)\n",
      "/mnt/z/data/BTCV//train/img/img0038.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 184)\n",
      "/mnt/z/data/BTCV//train/img/img0036.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 98)\n",
      "/mnt/z/data/BTCV//train/img/img0034.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 148)\n",
      "/mnt/z/data/BTCV//train/img/img0008.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 144)\n",
      "/mnt/z/data/BTCV//train/img/img0032.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 140)\n",
      "/mnt/z/data/BTCV//train/img/img0004.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 195)\n",
      "/mnt/z/data/BTCV//train/img/img0040.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 89)\n",
      "/mnt/z/data/BTCV//train/img/img0022.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 131)\n",
      "/mnt/z/data/BTCV//train/img/img0006.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n",
      "torch.Size([2, 10, 14, 224, 224])\n",
      "torch.Size([2, 10, 224, 224])\n",
      "(512, 512, 139)\n",
      "/mnt/z/data/BTCV//train/img/img0002.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "(512, 512, 124)\n",
      "/mnt/z/data/BTCV//train/img/img0024.nii\n",
      "after transform torch.Size([10, 3, 224, 224])\n",
      "torch.Size([2, 10, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m i \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m embeddings \u001b[39m=\u001b[39m feature_model(i)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m output \u001b[39m=\u001b[39m decoder(embeddings)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/eval/segmentation/utils.py:43\u001b[0m, in \u001b[0;36mDINOV2Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_3d:\n\u001b[0;32m---> 43\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_3d(x)\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_(x)\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/eval/segmentation/utils.py:26\u001b[0m, in \u001b[0;36mDINOV2Encoder.forward_3d\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     scans \u001b[39m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m     \u001b[39mfor\u001b[39;00m scan \u001b[39min\u001b[39;00m batch_scans:\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_zero_matrix(scan): scans\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_(scan\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)))\n\u001b[1;32m     27\u001b[0m     batch_features\u001b[39m.\u001b[39mappend(scans)\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m batch_features\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/eval/utils.py:436\u001b[0m, in \u001b[0;36mis_zero_matrix\u001b[0;34m(matrix)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_zero_matrix\u001b[39m(matrix):\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mallclose(matrix, torch\u001b[39m.\u001b[39;49mzeros_like(matrix))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True, image_size=224).cuda()\n",
    "# feature_model.eval()\n",
    "# decoder = UNetDecoder(in_channels=model.embed_dim, out_channels=3).cuda()\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=False)\n",
    "# feature_model_with_inter.eval()\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    print(i.shape)\n",
    "    embeddings = feature_model(i)\n",
    "    output = decoder(embeddings)\n",
    "    output = torch.stack(output, dim=0)\n",
    "    t = torch.stack(t, dim=0)\n",
    "    # print(t.unsqueeze(1).shape)\n",
    "    print(output.shape)\n",
    "    print(t.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_dataset.get_image_data(0)\n",
    "lbl = test_dataset.get_target(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOV2Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, autocast_ctx, is_3d=False) -> None:\n",
    "        super(DINOV2Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "        self.is_3d = is_3d\n",
    "    \n",
    "    def forward_3d(self, x):\n",
    "        batch_features = [] \n",
    "        for batch_scans in x: # calculate the features for every scan in all scans of the batch\n",
    "            scans = []\n",
    "            for scan in batch_scans:\n",
    "                if not is_zero_matrix(scan): scans.append(self.forward_(scan.unsqueeze(0)))\n",
    "            batch_features.append(scans)\n",
    "        return batch_features\n",
    "\n",
    "    def forward_(self, x):\n",
    "        with torch.no_grad():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.encoder.forward_features(x)['x_norm_patchtokens']\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if is_3d:\n",
    "            return self.forward_3d(x)\n",
    "        return self.forward_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in test_dataset:\n",
    "    show_image_from_tensor(i[0] * 100)\n",
    "    show_image_from_tensor(i[1] * 100)\n",
    "    show_image_from_tensor(i[2] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_results(feature_model, decoder, dataset):\n",
    "    for i, (img, _) in enumerate(dataset):\n",
    "\n",
    "        img_name = test_dataset.images[i]\n",
    "        _, affine_matrix = test_dataset.get_image_data(i, return_affine_matrix=True)\n",
    "\n",
    "        img = img.cuda(non_blocking=True) \n",
    "\n",
    "        features = feature_model(img.unsqueeze(0))\n",
    "        output = decoder(features, up_size=512)[0]\n",
    "        output = output.argmax(dim=1)\n",
    "\n",
    "        nifti_img = nib.Nifti1Image(output\n",
    "                                    .cpu()\n",
    "                                    .numpy()\n",
    "                                    .astype(np.uint8)\n",
    "                                    .transpose(1, 2, 0), affine_matrix)    \n",
    "        file_output_dir = test_results_path + os.sep + img_name + \".gz\"\n",
    "\n",
    "        # Save the NIfTI image\n",
    "        nib.save(nifti_img, file_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "save_test_results(f, ld, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "optimizer = torch.optim.SGD(ld.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda(non_blocking=True) \n",
    "\n",
    "    features = f(i)\n",
    "    output = ld(features)\n",
    "    \n",
    "    output = torch.cat(output, dim=0)\n",
    "    t = torch.cat(t, dim=0)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(output, t.cuda(non_blocking=True).type(torch.int64))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    optimizer.step()\n",
    "    # labels = t.view(-1, t.shape[-1], t.shape[-1])\n",
    "    # losses = nn.CrossEntropyLoss()(output.view(-1, 14, labels.shape[-1], labels.shape[-1]), labels)\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ld = LinearDecoder(in_channels=embed_dim, num_classes=3, is_3d=True)\n",
    "ld = ld.cuda()\n",
    "\n",
    "o = ld(features)\n",
    "print(len(o))\n",
    "print(o.shape)\n",
    "o = torch.stack([torch.nn.functional.interpolate(batch_output, size=448, mode=\"bilinear\", align_corners=False)\n",
    "                for batch_output in torch.unbind(o, dim=0)], dim=0)\n",
    "# ou = torch.nn.functional.interpolate(o[0], size=448, mode=\"bilinear\", align_corners=False)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
