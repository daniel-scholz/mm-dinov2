{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30392/1461348807.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchio\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchio'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "from typing import Any, Dict, Optional\n",
    "from collections import OrderedDict\n",
    "from monai.losses.dice import DiceLoss, DiceCELoss\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.wrappers import ClasswiseWrapper\n",
    "from torchmetrics.classification import (MultilabelAUROC, MultilabelF1Score, MultilabelAccuracy, MulticlassF1Score, \n",
    "                                        MulticlassAccuracy, MulticlassAUROC, Accuracy, BinaryF1Score, BinaryAUROC,\n",
    "                                        JaccardIndex, MulticlassJaccardIndex, Dice, BinaryAUROC)\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT, BTCV, BTCVSlice, AMOS, MSDHeart\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_padded_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets, make_data_loaders)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics, MetricAveraging, MetricType\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 80690/476234 [3:14:34<164:06:37,  1.49s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "os.environ['TMPDIR'] = 'D:/data/tmp'\n",
    "\n",
    "def resize_and_save_image(image_path, output_dir):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Resize the image\n",
    "        img = img.resize((1024, 1024))\n",
    "        # Create the same directory structure in the output directory\n",
    "        output_path = os.path.join(output_dir, image_path)\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        # Save the image\n",
    "        img.save(output_path)\n",
    "\n",
    "def process_chexpert_dataset(zip_file_path, output_dir):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        for filename in tqdm(zip_ref.namelist()):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                # Extract the file from the zip\n",
    "                zip_ref.extract(filename)\n",
    "                # Resize and save the image, then delete the original file\n",
    "                resize_and_save_image(filename, output_dir)\n",
    "                shutil.rmtree(\"C:/Users/user/Desktop/dinov2/CheXpert-v1.0\")\n",
    "\n",
    "# Call the function\n",
    "process_chexpert_dataset('Z:/Downloads/cheXpert.zip', 'D:/data/Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/z/data/amos22/\"\n",
    "train_data_path = data_dir + \"train/\"\n",
    "train_data = os.listdir(train_data_path)\n",
    "\n",
    "val_data_path = data_dir + \"val/\"\n",
    "val_data = os.listdir(val_data_path)\n",
    "\n",
    "test_data_path = data_dir + \"test/\"\n",
    "test_data = os.listdir(test_data_path)\n",
    "\n",
    "label_data_path = data_dir + \"labels/\"\n",
    "label_data = os.listdir(label_data_path)\n",
    "\n",
    "for label in label_data:\n",
    "    if label not in train_data or label not in val_data or label not in test_data:\n",
    "        os.remove(label_data_path + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/z/data/amos22/\"\n",
    "images_data_path = data_dir + \"images/\"\n",
    "images_data = os.listdir(images_data_path)\n",
    "\n",
    "train_data_path = data_dir + \"train\"\n",
    "val_data_path = data_dir + \"val\"\n",
    "test_data_path = data_dir + \"test\"\n",
    "label_data_path = data_dir + \"labels\"\n",
    "\n",
    "# os.makedirs(train_data_path, exist_ok=True)\n",
    "# train_images = np.arange(0, 300, 300/80).round().astype(\"int\")\n",
    "# os.makedirs(val_data_path, exist_ok=True)\n",
    "# val_images = np.arange(0, 220, 220/20).round().astype(\"int\")\n",
    "# os.makedirs(test_data_path, exist_ok=True)\n",
    "# test_images = np.arange(0, 200, 200/50).round().astype(\"int\")\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in train_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, train_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in val_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, val_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in test_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, test_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "train_data = os.listdir(train_data_path)\n",
    "val_data = os.listdir(val_data_path)\n",
    "test_data = os.listdir(test_data_path)\n",
    "label_data = os.listdir(label_data_path)\n",
    "\n",
    "for scan in label_data:\n",
    "    if scan not in train_data and scan not in val_data and scan not in test_data:\n",
    "        os.remove(label_data_path + os.sep + scan)\n",
    "\n",
    "for scan in train_data:\n",
    "    nifit_scan = nib.load(train_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    scan_name = scan.split(\".\")[0]\n",
    "\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(train_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(train_data_path + os.sep + scan)\n",
    "\n",
    "for scan in val_data:\n",
    "    nifit_scan = nib.load(val_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    \n",
    "    scan_name = scan.split(\".\")[0]\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(val_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(val_data_path + os.sep + scan)\n",
    "\n",
    "for scan in test_data:\n",
    "    nifit_scan = nib.load(test_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    \n",
    "    scan_name = scan.split(\".\")[0]\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(test_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(test_data_path + os.sep + scan)\n",
    "\n",
    "for scan in label_data:\n",
    "    nifit_scan = nib.load(label_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    scan_name = scan.split(\".\")[0]\n",
    "\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(label_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(label_data_path + os.sep + scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import SwinUNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20231113 11:30:32 1422 dinov2 config.py:60] git:\n",
      "  sha: 6281d97e691a8518e65e181f36610cd83f453980, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20231113 11:30:32 1422 dinov2 config.py:61] backbone: dinov2\n",
      "batch_size: 8\n",
      "comment: \n",
      "config_file: dinov2/configs/eval/vitl14_pretrain.yaml\n",
      "exclude: \n",
      "gather_on_cpu: False\n",
      "n_per_class_list: [-1]\n",
      "n_tries: 1\n",
      "nb_knn: [5, 20, 50, 100, 200]\n",
      "ngpus: 1\n",
      "nodes: 1\n",
      "opts: ['train.output_dir=/mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vitl14/knn']\n",
      "output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vitl14/knn\n",
      "partition: learnlab\n",
      "pretrained_weights: models/dinov2_vitl14_pretrain.pth\n",
      "temperature: 0.07\n",
      "test_dataset_str: BTCV:split=VAL:root=/mnt/z/data/BTCV\n",
      "timeout: 2800\n",
      "train_dataset_str: BTCV:split=TRAIN:root=/mnt/z/data/BTCV\n",
      "use_volta32: False\n",
      "I20231113 11:30:32 1422 dinov2 config.py:27] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20231113 11:30:32 1422 dinov2 config.py:34] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: ImageNet:split=TRAIN\n",
      "  output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vitl14/knn\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_large\n",
      "  patch_size: 14\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 518\n",
      "  local_crops_size: 98\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "I20231113 11:30:32 1422 dinov2 vision_transformer.py:110] using MLP layer as FFN\n",
      "I20231113 11:31:53 1422 dinov2 utils.py:35] Pretrained weights found at models/dinov2_vitl14_pretrain.pth and loaded with msg: <All keys matched successfully>\n",
      "I20231113 11:31:53 1422 dinov2 setup.py:88] Using DINOv2 backbone\n"
     ]
    }
   ],
   "source": [
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/mnt/d/data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='MC:split=TRAIN:root=/mnt/z/data/MC', val_dataset_str='MC:split=VAL:root=/mnt/z/data/MC', test_dataset_str='MC:split=TEST:root=/mnt/z/data/MC', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "args = argparse.Namespace(backbone=\"dinov2\", config_file='dinov2/configs/eval/vitl14_pretrain.yaml', pretrained_weights='models/dinov2_vitl14_pretrain.pth', output_dir='results/NIH/dinov2_vitl14/knn', opts=[], train_dataset_str='BTCV:split=TRAIN:root=/mnt/z/data/BTCV', test_dataset_str='BTCV:split=VAL:root=/mnt/z/data/BTCV', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 4, autocast_ctx, is_3d=False)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "from dinov2.eval.segmentation.utils import UNetDecoder\n",
    "m = UNetDecoder(1536, out_channels=1, image_size=448)\n",
    "decoder = LinearDecoder(\n",
    "    1024, num_classes=1, image_size=448, patch_size=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier = LinearClassifier(\n",
    "    1536, use_n_blocks=1, use_avgpool=False, num_classes=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA trainable params: 14350 || all params: 14350 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "from dinov2.eval.utils import trainable_parameters\n",
    "tp, ap = trainable_parameters(decoder)\n",
    "print(f\"LoRA trainable params: {tp} || all params: {ap} || trainable%: {100 * tp / ap:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "from transformers import (\n",
    "    SamVisionConfig,\n",
    "    SamPromptEncoderConfig,\n",
    "    SamMaskDecoderConfig,\n",
    "    SamModel,\n",
    "    SamConfig,\n",
    ")\n",
    "\n",
    "config = SamConfig(SamVisionConfig(image_size=224))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-large\", config=config, ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.eval.utils import bitfit\n",
    "model = bitfit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_total_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "images = BTCVSlice(root=\"/mnt/z/data/BTCVSlice/\", split=_Split.TRAIN, transform=train_image_transform, target_transform=train_target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    print(i.shape)\n",
    "    l = model.get_image_embeddings(i[0].unsqueeze(0))\n",
    "    print(l.shape)\n",
    "    print(np.unique(t))\n",
    "    print(t.max())\n",
    "    show_image_from_tensor(t[6].unsqueeze(0) * 50 )\n",
    "    print(images.get_num_classes())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "# train_image_transform = make_classification_train_transform()\n",
    "# eval_image_transform = make_classification_eval_transform()\n",
    "\n",
    "# train_target_transform = eval_target_transform = None\n",
    "\n",
    "# val_dataset_str = args.val_dataset_str\n",
    "val_dataset_str = None\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=args.train_dataset_str, val_dataset_str=val_dataset_str,\n",
    "                                                        test_dataset_str=args.test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "is_3d = test_dataset.is_3d()\n",
    "collate_fn=collate_fn_3d if is_3d else None\n",
    "start_iter=1\n",
    "sampler_type = SamplerType.INFINITE\n",
    "# sampler_type = None\n",
    "seed = 0\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "train_data_loader, val_data_loader, test_data_loader = make_data_loaders(train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                                                                        val_dataset=val_dataset, sampler_type=sampler_type, seed=seed,\n",
    "                                                                        start_iter=start_iter, batch_size=batch_size, num_workers=num_workers,\n",
    "                                                                        collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=train_dataset,\n",
    "#     batch_size=2,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=images,\n",
    "#     batch_size=4,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "# val_data_loader = make_data_loader(\n",
    "#     dataset=val_dataset,\n",
    "#     batch_size=4,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=None,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3DWrapper(nn.Module):\n",
    "    def __init__(self, model, per_slice=False) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.per_slice = per_slice\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_outputs = []\n",
    "        for slices in x: \n",
    "            if self.per_slice:\n",
    "                batch_outputs.append(\n",
    "                    torch.stack([self.model(slice_) for slice_ in slices], dim=0).squeeze()\n",
    "                )\n",
    "            else:\n",
    "                batch_outputs.append(\n",
    "                    self.model(slices)\n",
    "                )\n",
    "        return batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset.class_names\n",
    "num_of_classes = train_dataset.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build_segmentation_metrics(MetricType.SEGMENTATION_METRICS.accuracy_averaging, num_labels=7, labels=labels.tolist())\n",
    "m = m.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, lbl1 = test_dataset[0]\n",
    "slice1, slice1lbl = img1[130].cuda(non_blocking=True), lbl1[130].cuda(non_blocking=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = slice1.unsqueeze(0)\n",
    "slice1lbl = slice1lbl.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = slice1.type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=False).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=7, image_size=224).cuda()\n",
    "# decoder = Model3DWrapper(decoder, per_slice=True)\n",
    "optimizer = torch.optim.SGD(decoder.parameters(), lr=3.5e-4, momentum=0.9, weight_decay=0)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "for i in range(1000):\n",
    "    o1 = feature_model(slice1)\n",
    "    o2 = decoder(o1)\n",
    "\n",
    "    \n",
    "    o2 = o2.type(torch.float16)\n",
    "    print(slice1lbl.dtype)\n",
    "    loss = loss_function(o2, slice1lbl)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        # o2 = torch.argmax(o2, dim=1)\n",
    "        print(loss)\n",
    "        # print(m(o2, slice1lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.logging import MetricLogger\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = Model3DWrapper(LinearDecoder(in_channels=model.embed_dim, num_classes=14, image_size=224).cuda(), per_slice=True)\n",
    "        \n",
    "for samples, targets in train_data_loader:\n",
    "\n",
    "    samples = samples.cuda(non_blocking=True)\n",
    "\n",
    "    o1 = feature_model(samples)\n",
    "    # o2 = decoder(o1)\n",
    "\n",
    "    # o2 = torch.cat(o2, dim=0).cuda()\n",
    "    # targets = torch.cat(targets, dim=0).cuda()\n",
    "    # preds = o2.argmax(dim=1)\n",
    "    # targets = targets.type(torch.int64)\n",
    "\n",
    "    # print(o2.shape)\n",
    "    # print(targets.shape)\n",
    "\n",
    "    # z = {\n",
    "    #     \"preds\": preds[50:80],\n",
    "    #     \"target\": targets[50:80],\n",
    "    # }\n",
    "\n",
    "    # print(m.update(**z))\n",
    "    # print(m)\n",
    "    # print(m.compute())\n",
    "\n",
    "    break \n",
    "    # print(t.shape)\n",
    "    # print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoderUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, embed_dim=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*2, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.skip_conv = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upconv(x1)\n",
    "        x2 = self.skip_conv(x2)\n",
    "        scale_factor = (x1.size()[2] / x2.size()[2])\n",
    "        x2 = nn.Upsample(scale_factor=scale_factor, mode=\"bilinear\", align_corners=True)(x2)\n",
    "        x = torch.concat([x1, x2], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, image_size=224):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.embed_dim = in_channels\n",
    "        self.image_size = image_size\n",
    "        self.out_channels = out_channels\n",
    "        self.up1 = UNetDecoderUpBlock(in_channels=in_channels, out_channels=in_channels//2, embed_dim=embed_dim)\n",
    "        self.up2 = UNetDecoderUpBlock(in_channels=in_channels//2, out_channels=in_channels//4, embed_dim=embed_dim)\n",
    "        self.up3 = UNetDecoderUpBlock(in_channels=in_channels//4, out_channels=in_channels//8, embed_dim=embed_dim)\n",
    "        self.up4 = UNetDecoderUpBlock(in_channels=in_channels//8, out_channels=out_channels, embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = w = self.image_size//14\n",
    "\n",
    "        skip1 = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip2 = x[2].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip3 = x[1].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip4 = x[0].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        x1    = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        \n",
    "        x2 = self.up1(x1, skip1)\n",
    "        x3 = self.up2(x2, skip2)\n",
    "        x4 = self.up3(x3, skip3)\n",
    "        x5 = self.up4(x4, skip4)\n",
    "\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    print(i.shape)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True, image_size=224).cuda()\n",
    "# feature_model.eval()\n",
    "# decoder = UNetDecoder(in_channels=model.embed_dim, out_channels=3).cuda()\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=False)\n",
    "# feature_model_with_inter.eval()\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    print(i.shape)\n",
    "    embeddings = feature_model(i)\n",
    "    output = decoder(embeddings)\n",
    "    output = torch.stack(output, dim=0)\n",
    "    t = torch.stack(t, dim=0)\n",
    "    # print(t.unsqueeze(1).shape)\n",
    "    print(output.shape)\n",
    "    print(t.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_dataset.get_image_data(0)\n",
    "lbl = test_dataset.get_target(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOV2Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, autocast_ctx, is_3d=False) -> None:\n",
    "        super(DINOV2Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "        self.is_3d = is_3d\n",
    "    \n",
    "    def forward_3d(self, x):\n",
    "        batch_features = [] \n",
    "        for batch_scans in x: # calculate the features for every scan in all scans of the batch\n",
    "            scans = []\n",
    "            for scan in batch_scans:\n",
    "                if not is_zero_matrix(scan): scans.append(self.forward_(scan.unsqueeze(0)))\n",
    "            batch_features.append(scans)\n",
    "        return batch_features\n",
    "\n",
    "    def forward_(self, x):\n",
    "        with torch.no_grad():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.encoder.forward_features(x)['x_norm_patchtokens']\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if is_3d:\n",
    "            return self.forward_3d(x)\n",
    "        return self.forward_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in test_dataset:\n",
    "    show_image_from_tensor(i[0] * 100)\n",
    "    show_image_from_tensor(i[1] * 100)\n",
    "    show_image_from_tensor(i[2] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_results(feature_model, decoder, dataset):\n",
    "    for i, (img, _) in enumerate(dataset):\n",
    "\n",
    "        img_name = test_dataset.images[i]\n",
    "        _, affine_matrix = test_dataset.get_image_data(i, return_affine_matrix=True)\n",
    "\n",
    "        img = img.cuda(non_blocking=True) \n",
    "\n",
    "        features = feature_model(img.unsqueeze(0))\n",
    "        output = decoder(features, up_size=512)[0]\n",
    "        output = output.argmax(dim=1)\n",
    "\n",
    "        nifti_img = nib.Nifti1Image(output\n",
    "                                    .cpu()\n",
    "                                    .numpy()\n",
    "                                    .astype(np.uint8)\n",
    "                                    .transpose(1, 2, 0), affine_matrix)    \n",
    "        file_output_dir = test_results_path + os.sep + img_name + \".gz\"\n",
    "\n",
    "        # Save the NIfTI image\n",
    "        nib.save(nifti_img, file_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "save_test_results(f, ld, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "optimizer = torch.optim.SGD(ld.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda(non_blocking=True) \n",
    "\n",
    "    features = f(i)\n",
    "    output = ld(features)\n",
    "    \n",
    "    output = torch.cat(output, dim=0)\n",
    "    t = torch.cat(t, dim=0)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(output, t.cuda(non_blocking=True).type(torch.int64))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    optimizer.step()\n",
    "    # labels = t.view(-1, t.shape[-1], t.shape[-1])\n",
    "    # losses = nn.CrossEntropyLoss()(output.view(-1, 14, labels.shape[-1], labels.shape[-1]), labels)\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ld = LinearDecoder(in_channels=embed_dim, num_classes=3, is_3d=True)\n",
    "ld = ld.cuda()\n",
    "\n",
    "o = ld(features)\n",
    "print(len(o))\n",
    "print(o.shape)\n",
    "o = torch.stack([torch.nn.functional.interpolate(batch_output, size=448, mode=\"bilinear\", align_corners=False)\n",
    "                for batch_output in torch.unbind(o, dim=0)], dim=0)\n",
    "# ou = torch.nn.functional.interpolate(o[0], size=448, mode=\"bilinear\", align_corners=False)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
