{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "from typing import Any, Dict, Optional\n",
    "from collections import OrderedDict\n",
    "from monai.losses.dice import DiceLoss, DiceCELoss\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.wrappers import ClasswiseWrapper\n",
    "from torchmetrics.classification import (MultilabelAUROC, MultilabelF1Score, MultilabelAccuracy, MulticlassF1Score, \n",
    "                                        MulticlassAccuracy, MulticlassAUROC, Accuracy, BinaryF1Score, BinaryAUROC,\n",
    "                                        JaccardIndex, MulticlassJaccardIndex, Dice, BinaryAUROC)\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT, BTCV, BTCVSlice\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_padded_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets, make_data_loaders)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics, MetricAveraging, MetricType\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import SwinUNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input image size (img_size) should be divisible by stage-wise image resolution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m SwinUNETR(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     img_size\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m96\u001b[39;49m, \u001b[39m96\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     in_channels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     out_channels\u001b[39m=\u001b[39;49m\u001b[39m14\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     feature_size\u001b[39m=\u001b[39;49m\u001b[39m48\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     use_checkpoint\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/nets/swin_unetr.py:115\u001b[0m, in \u001b[0;36mSwinUNETR.__init__\u001b[0;34m(self, img_size, in_channels, out_channels, depths, num_heads, feature_size, norm_name, drop_rate, attn_drop_rate, dropout_path_rate, normalize, use_checkpoint, spatial_dims, downsample, use_v2)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m    114\u001b[0m         \u001b[39mif\u001b[39;00m m \u001b[39m%\u001b[39m np\u001b[39m.\u001b[39mpower(p, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39minput image size (img_size) should be divisible by stage-wise image resolution.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m drop_rate \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout rate should be between 0 and 1.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: input image size (img_size) should be divisible by stage-wise image resolution."
     ]
    }
   ],
   "source": [
    "model = SwinUNETR(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=1,\n",
    "    out_channels=14,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True,\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20231027 23:40:49 233 dinov2 config.py:60] git:\n",
      "  sha: 74ef31f70e52cd45386edab8b5e4c7cfd42b23c5, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20231027 23:40:49 233 dinov2 config.py:61] backbone: dinov2\n",
      "batch_size: 8\n",
      "comment: \n",
      "config_file: dinov2/configs/eval/vits14_pretrain.yaml\n",
      "exclude: \n",
      "gather_on_cpu: False\n",
      "n_per_class_list: [-1]\n",
      "n_tries: 1\n",
      "nb_knn: [5, 20, 50, 100, 200]\n",
      "ngpus: 1\n",
      "nodes: 1\n",
      "opts: ['train.output_dir=/mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn']\n",
      "output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "partition: learnlab\n",
      "pretrained_weights: models/dinov2_vits14_pretrain.pth\n",
      "temperature: 0.07\n",
      "test_dataset_str: BTCV:split=VAL:root=/mnt/z/data/BTCV\n",
      "timeout: 2800\n",
      "train_dataset_str: BTCV:split=TRAIN:root=/mnt/z/data/BTCV\n",
      "use_volta32: False\n",
      "I20231027 23:40:49 233 dinov2 config.py:27] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20231027 23:40:49 233 dinov2 config.py:34] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: ImageNet:split=TRAIN\n",
      "  output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_small\n",
      "  patch_size: 14\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 518\n",
      "  local_crops_size: 98\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "I20231027 23:40:49 233 dinov2 vision_transformer.py:110] using MLP layer as FFN\n",
      "I20231027 23:40:50 233 dinov2 utils.py:35] Pretrained weights found at models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>\n",
      "I20231027 23:40:50 233 dinov2 setup.py:84] Using DINOv2 backbone\n"
     ]
    }
   ],
   "source": [
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/mnt/d/data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='MC:split=TRAIN:root=/mnt/z/data/MC', val_dataset_str='MC:split=VAL:root=/mnt/z/data/MC', test_dataset_str='MC:split=TEST:root=/mnt/z/data/MC', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "args = argparse.Namespace(backbone=\"dinov2\", config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='BTCV:split=TRAIN:root=/mnt/z/data/BTCV', test_dataset_str='BTCV:split=VAL:root=/mnt/z/data/BTCV', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 4, autocast_ctx, is_3d=False)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained('google/vit-large-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SamModel were not initialized from the model checkpoint at facebook/sam-vit-large and are newly initialized because the shapes did not match:\n",
      "- vision_encoder.pos_embed: found shape torch.Size([1, 64, 64, 1024]) in the checkpoint and torch.Size([1, 14, 14, 768]) in the model instantiated\n",
      "- vision_encoder.patch_embed.projection.weight: found shape torch.Size([1024, 3, 16, 16]) in the checkpoint and torch.Size([768, 3, 16, 16]) in the model instantiated\n",
      "- vision_encoder.patch_embed.projection.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.0.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.0.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.0.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.0.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.0.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.0.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.0.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.0.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.0.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.0.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.0.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.0.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.1.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.1.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.1.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.1.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.1.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.1.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.1.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.1.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.1.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.1.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.1.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.1.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.2.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.2.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.2.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.2.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.2.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.2.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.2.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.2.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.2.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.3.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.3.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.3.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.3.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.3.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.3.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.3.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.3.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.3.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.3.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.3.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.3.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.4.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.4.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.4.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.4.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.4.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.4.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.4.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.4.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.4.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.4.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.4.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.4.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.5.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.5.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([27, 64]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([27, 64]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.5.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.5.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.5.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.5.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.5.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.5.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.5.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.6.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.6.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.6.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.6.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.6.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.6.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.6.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.6.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.6.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.6.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.6.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.6.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.7.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.7.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.7.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.7.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.7.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.7.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.7.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.7.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.7.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.7.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.7.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.7.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.8.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.8.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.8.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.8.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.8.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.8.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.8.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.8.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.8.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.9.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.9.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.9.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.9.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.9.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.9.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.9.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.9.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.9.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.9.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.9.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.9.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.10.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.10.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.10.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.10.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.10.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.10.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.10.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.10.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.10.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.10.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.10.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.10.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.11.layer_norm1.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.11.layer_norm1.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.rel_pos_h: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([27, 64]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.rel_pos_w: found shape torch.Size([127, 64]) in the checkpoint and torch.Size([27, 64]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.qkv.weight: found shape torch.Size([3072, 1024]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.qkv.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- vision_encoder.layers.11.attn.proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.11.layer_norm2.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.11.layer_norm2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.layers.11.mlp.lin1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- vision_encoder.layers.11.mlp.lin1.bias: found shape torch.Size([4096]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
      "- vision_encoder.layers.11.mlp.lin2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- vision_encoder.layers.11.mlp.lin2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- vision_encoder.neck.conv1.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([256, 768, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "from transformers import (\n",
    "    SamVisionConfig,\n",
    "    SamPromptEncoderConfig,\n",
    "    SamMaskDecoderConfig,\n",
    "    SamModel,\n",
    "    SamConfig,\n",
    ")\n",
    "\n",
    "config = SamConfig(SamVisionConfig(image_size=224))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-large\", config=config, ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 51456 || all params: 22056576 || trainable%: 0.23\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_total_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "images = BTCVSlice(root=\"/mnt/z/data/BTCVSlice/\", split=_Split.TRAIN, transform=train_image_transform, target_transform=train_target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamVisionEncoder(\n",
       "  (patch_embed): SamPatchEmbeddings(\n",
       "    (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x SamVisionLayer(\n",
       "      (layer_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): SamVisionAttention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SamMLPBlock(\n",
       "        (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): SamVisionNeck(\n",
       "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (layer_norm1): SamLayerNorm()\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer_norm2): SamLayerNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vision_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input image size (224*224) doesn't match model (1024*1024).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m train_dataset:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     l \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_image_embeddings(i[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(l\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39munique(t))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:1243\u001b[0m, in \u001b[0;36mSamModel.get_image_embeddings\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m   1222\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_image_embeddings\u001b[39m(\n\u001b[1;32m   1223\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     return_dict: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1228\u001b[0m ):\n\u001b[1;32m   1229\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[39m    Returns the image embeddings by passing the pixel values through the vision encoder.\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \n\u001b[1;32m   1242\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m     vision_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_encoder(\n\u001b[1;32m   1244\u001b[0m         pixel_values,\n\u001b[1;32m   1245\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1246\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1247\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[1;32m   1249\u001b[0m     image_embeddings \u001b[39m=\u001b[39m vision_output[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1250\u001b[0m     \u001b[39mreturn\u001b[39;00m image_embeddings\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:1033\u001b[0m, in \u001b[0;36mSamVisionEncoder.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m pixel_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1031\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1033\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embed(pixel_values)\n\u001b[1;32m   1034\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:142\u001b[0m, in \u001b[0;36mSamPatchEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMake sure that the channel dimension of the pixel values match with the one set in the configuration.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m height \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m width \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image size (\u001b[39m\u001b[39m{\u001b[39;00mheight\u001b[39m}\u001b[39;00m\u001b[39m*\u001b[39m\u001b[39m{\u001b[39;00mwidth\u001b[39m}\u001b[39;00m\u001b[39m) doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match model (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m*\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(pixel_values)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mValueError\u001b[0m: Input image size (224*224) doesn't match model (1024*1024)."
     ]
    }
   ],
   "source": [
    "for i, t in train_dataset:\n",
    "    print(i.shape)\n",
    "    l = model.get_image_embeddings(i[0].unsqueeze(0))\n",
    "    print(l.shape)\n",
    "    print(np.unique(t))\n",
    "    print(t.max())\n",
    "    show_image_from_tensor(t[6].unsqueeze(0) * 50 )\n",
    "    print(images.get_num_classes())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20231028 00:01:12 233 dinov2 loaders.py:101] using dataset: \"BTCV:split=TRAIN:root=/mnt/z/data/BTCV\"\n",
      "I20231028 00:01:12 233 root btcv.py:65] 0 scans are missing from TRAIN set\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:106] # of dataset samples: 15\n",
      "I20231028 00:01:12 233 dinov2 utils.py:446] Train and val datasets have been combined.\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:101] using dataset: \"BTCV:split=VAL:root=/mnt/z/data/BTCV\"\n",
      "I20231028 00:01:12 233 root btcv.py:65] 0 scans are missing from VAL set\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:106] # of dataset samples: 15\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:129] sampler: infinite\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:223] using PyTorch data loader\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:238] infinite data loader\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:176] sampler: none\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:223] using PyTorch data loader\n",
      "I20231028 00:01:12 233 dinov2 loaders.py:236] # of batches: 15\n"
     ]
    }
   ],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "# train_image_transform = make_classification_train_transform()\n",
    "# eval_image_transform = make_classification_eval_transform()\n",
    "\n",
    "# train_target_transform = eval_target_transform = None\n",
    "\n",
    "# val_dataset_str = args.val_dataset_str\n",
    "val_dataset_str = None\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=args.train_dataset_str, val_dataset_str=val_dataset_str,\n",
    "                                                        test_dataset_str=args.test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "is_3d = test_dataset.is_3d()\n",
    "collate_fn=collate_fn_3d if is_3d else None\n",
    "start_iter=1\n",
    "sampler_type = SamplerType.INFINITE\n",
    "# sampler_type = None\n",
    "seed = 0\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "train_data_loader, val_data_loader, test_data_loader = make_data_loaders(train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                                                                        val_dataset=val_dataset, sampler_type=sampler_type, seed=seed,\n",
    "                                                                        start_iter=start_iter, batch_size=batch_size, num_workers=num_workers,\n",
    "                                                                        collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=train_dataset,\n",
    "#     batch_size=2,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=images,\n",
    "#     batch_size=4,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "# val_data_loader = make_data_loader(\n",
    "#     dataset=val_dataset,\n",
    "#     batch_size=4,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=None,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3DWrapper(nn.Module):\n",
    "    def __init__(self, model, per_slice=False) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.per_slice = per_slice\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_outputs = []\n",
    "        for slices in x: \n",
    "            if self.per_slice:\n",
    "                batch_outputs.append(\n",
    "                    torch.stack([self.model(slice_) for slice_ in slices], dim=0).squeeze()\n",
    "                )\n",
    "            else:\n",
    "                batch_outputs.append(\n",
    "                    self.model(slices)\n",
    "                )\n",
    "        return batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset.class_names\n",
    "num_of_classes = train_dataset.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build_segmentation_metrics(MetricType.SEGMENTATION_METRICS.accuracy_averaging, num_labels=7, labels=labels.tolist())\n",
    "m = m.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, lbl1 = test_dataset[0]\n",
    "slice1, slice1lbl = img1[130].cuda(non_blocking=True), lbl1[130].cuda(non_blocking=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = slice1.unsqueeze(0)\n",
    "slice1lbl = slice1lbl.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = slice1.type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=False).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=7, image_size=224).cuda()\n",
    "# decoder = Model3DWrapper(decoder, per_slice=True)\n",
    "optimizer = torch.optim.SGD(decoder.parameters(), lr=3.5e-4, momentum=0.9, weight_decay=0)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "for i in range(1000):\n",
    "    o1 = feature_model(slice1)\n",
    "    o2 = decoder(o1)\n",
    "\n",
    "    \n",
    "    o2 = o2.type(torch.float16)\n",
    "    print(slice1lbl.dtype)\n",
    "    loss = loss_function(o2, slice1lbl)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        # o2 = torch.argmax(o2, dim=1)\n",
    "        print(loss)\n",
    "        # print(m(o2, slice1lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.logging import MetricLogger\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = Model3DWrapper(LinearDecoder(in_channels=model.embed_dim, num_classes=14, image_size=224).cuda(), per_slice=True)\n",
    "        \n",
    "for samples, targets in train_data_loader:\n",
    "\n",
    "    samples = samples.cuda(non_blocking=True)\n",
    "\n",
    "    o1 = feature_model(samples)\n",
    "    # o2 = decoder(o1)\n",
    "\n",
    "    # o2 = torch.cat(o2, dim=0).cuda()\n",
    "    # targets = torch.cat(targets, dim=0).cuda()\n",
    "    # preds = o2.argmax(dim=1)\n",
    "    # targets = targets.type(torch.int64)\n",
    "\n",
    "    # print(o2.shape)\n",
    "    # print(targets.shape)\n",
    "\n",
    "    # z = {\n",
    "    #     \"preds\": preds[50:80],\n",
    "    #     \"target\": targets[50:80],\n",
    "    # }\n",
    "\n",
    "    # print(m.update(**z))\n",
    "    # print(m)\n",
    "    # print(m.compute())\n",
    "\n",
    "    break \n",
    "    # print(t.shape)\n",
    "    # print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoderUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, embed_dim=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*2, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.skip_conv = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upconv(x1)\n",
    "        x2 = self.skip_conv(x2)\n",
    "        scale_factor = (x1.size()[2] / x2.size()[2])\n",
    "        x2 = nn.Upsample(scale_factor=scale_factor, mode=\"bilinear\", align_corners=True)(x2)\n",
    "        x = torch.concat([x1, x2], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, image_size=224):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.embed_dim = in_channels\n",
    "        self.image_size = image_size\n",
    "        self.out_channels = out_channels\n",
    "        self.up1 = UNetDecoderUpBlock(in_channels=in_channels, out_channels=in_channels//2, embed_dim=embed_dim)\n",
    "        self.up2 = UNetDecoderUpBlock(in_channels=in_channels//2, out_channels=in_channels//4, embed_dim=embed_dim)\n",
    "        self.up3 = UNetDecoderUpBlock(in_channels=in_channels//4, out_channels=in_channels//8, embed_dim=embed_dim)\n",
    "        self.up4 = UNetDecoderUpBlock(in_channels=in_channels//8, out_channels=out_channels, embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = w = self.image_size//14\n",
    "\n",
    "        skip1 = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip2 = x[2].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip3 = x[1].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip4 = x[0].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        x1    = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        \n",
    "        x2 = self.up1(x1, skip1)\n",
    "        x3 = self.up2(x2, skip2)\n",
    "        x4 = self.up3(x3, skip3)\n",
    "        x5 = self.up4(x4, skip4)\n",
    "\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    print(i.shape)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True, image_size=224).cuda()\n",
    "# feature_model.eval()\n",
    "# decoder = UNetDecoder(in_channels=model.embed_dim, out_channels=3).cuda()\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=False)\n",
    "# feature_model_with_inter.eval()\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    print(i.shape)\n",
    "    embeddings = feature_model(i)\n",
    "    output = decoder(embeddings)\n",
    "    output = torch.stack(output, dim=0)\n",
    "    t = torch.stack(t, dim=0)\n",
    "    # print(t.unsqueeze(1).shape)\n",
    "    print(output.shape)\n",
    "    print(t.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_dataset.get_image_data(0)\n",
    "lbl = test_dataset.get_target(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOV2Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, autocast_ctx, is_3d=False) -> None:\n",
    "        super(DINOV2Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "        self.is_3d = is_3d\n",
    "    \n",
    "    def forward_3d(self, x):\n",
    "        batch_features = [] \n",
    "        for batch_scans in x: # calculate the features for every scan in all scans of the batch\n",
    "            scans = []\n",
    "            for scan in batch_scans:\n",
    "                if not is_zero_matrix(scan): scans.append(self.forward_(scan.unsqueeze(0)))\n",
    "            batch_features.append(scans)\n",
    "        return batch_features\n",
    "\n",
    "    def forward_(self, x):\n",
    "        with torch.no_grad():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.encoder.forward_features(x)['x_norm_patchtokens']\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if is_3d:\n",
    "            return self.forward_3d(x)\n",
    "        return self.forward_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in test_dataset:\n",
    "    show_image_from_tensor(i[0] * 100)\n",
    "    show_image_from_tensor(i[1] * 100)\n",
    "    show_image_from_tensor(i[2] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_results(feature_model, decoder, dataset):\n",
    "    for i, (img, _) in enumerate(dataset):\n",
    "\n",
    "        img_name = test_dataset.images[i]\n",
    "        _, affine_matrix = test_dataset.get_image_data(i, return_affine_matrix=True)\n",
    "\n",
    "        img = img.cuda(non_blocking=True) \n",
    "\n",
    "        features = feature_model(img.unsqueeze(0))\n",
    "        output = decoder(features, up_size=512)[0]\n",
    "        output = output.argmax(dim=1)\n",
    "\n",
    "        nifti_img = nib.Nifti1Image(output\n",
    "                                    .cpu()\n",
    "                                    .numpy()\n",
    "                                    .astype(np.uint8)\n",
    "                                    .transpose(1, 2, 0), affine_matrix)    \n",
    "        file_output_dir = test_results_path + os.sep + img_name + \".gz\"\n",
    "\n",
    "        # Save the NIfTI image\n",
    "        nib.save(nifti_img, file_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "save_test_results(f, ld, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "optimizer = torch.optim.SGD(ld.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda(non_blocking=True) \n",
    "\n",
    "    features = f(i)\n",
    "    output = ld(features)\n",
    "    \n",
    "    output = torch.cat(output, dim=0)\n",
    "    t = torch.cat(t, dim=0)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(output, t.cuda(non_blocking=True).type(torch.int64))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    optimizer.step()\n",
    "    # labels = t.view(-1, t.shape[-1], t.shape[-1])\n",
    "    # losses = nn.CrossEntropyLoss()(output.view(-1, 14, labels.shape[-1], labels.shape[-1]), labels)\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ld = LinearDecoder(in_channels=embed_dim, num_classes=3, is_3d=True)\n",
    "ld = ld.cuda()\n",
    "\n",
    "o = ld(features)\n",
    "print(len(o))\n",
    "print(o.shape)\n",
    "o = torch.stack([torch.nn.functional.interpolate(batch_output, size=448, mode=\"bilinear\", align_corners=False)\n",
    "                for batch_output in torch.unbind(o, dim=0)], dim=0)\n",
    "# ou = torch.nn.functional.interpolate(o[0], size=448, mode=\"bilinear\", align_corners=False)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
