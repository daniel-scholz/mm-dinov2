{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_zero_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Distributed mode has already been enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m args \u001b[39m=\u001b[39m argparse\u001b[39m.\u001b[39mNamespace(config_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdinov2/configs/eval/vits14_pretrain.yaml\u001b[39m\u001b[39m'\u001b[39m, pretrained_weights\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/dinov2_vits14_pretrain.pth\u001b[39m\u001b[39m'\u001b[39m, output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresults/NIH/dinov2_vits14/knn\u001b[39m\u001b[39m'\u001b[39m, opts\u001b[39m=\u001b[39m[], train_dataset_str\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBTCV:split=TRAIN:root=/mnt/z/data/Abdomen/RawData\u001b[39m\u001b[39m'\u001b[39m, val_dataset_str\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBTCV:split=VAL:root=/mnt/z/data/Abdomen/RawData\u001b[39m\u001b[39m'\u001b[39m, test_dataset_str\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBTCV:split=TEST:root=/mnt/z/data/Abdomen/RawData\u001b[39m\u001b[39m'\u001b[39m, nb_knn\u001b[39m=\u001b[39m[\u001b[39m5\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m200\u001b[39m], temperature\u001b[39m=\u001b[39m\u001b[39m0.07\u001b[39m, gather_on_cpu\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, n_per_class_list\u001b[39m=\u001b[39m[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], n_tries\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, ngpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, nodes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39m2800\u001b[39m, partition\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlearnlab\u001b[39m\u001b[39m'\u001b[39m, use_volta32\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, comment\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, exclude\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model, autocast_dtype \u001b[39m=\u001b[39m setup_and_build_model(args)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m autocast_ctx \u001b[39m=\u001b[39m partial(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast, enabled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dtype\u001b[39m=\u001b[39mautocast_dtype)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# feature_model = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=True)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# model = ModelWithNormalize(model)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/eval/setup.py:73\u001b[0m, in \u001b[0;36msetup_and_build_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_and_build_model\u001b[39m(args) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, torch\u001b[39m.\u001b[39mdtype]:\n\u001b[1;32m     72\u001b[0m     cudnn\u001b[39m.\u001b[39mbenchmark \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     config \u001b[39m=\u001b[39m setup(args)\n\u001b[1;32m     74\u001b[0m     model \u001b[39m=\u001b[39m build_model_for_eval(config, args\u001b[39m.\u001b[39mpretrained_weights)\n\u001b[1;32m     75\u001b[0m     autocast_dtype \u001b[39m=\u001b[39m get_autocast_dtype(config)\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/utils/config.py:70\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     68\u001b[0m cfg \u001b[39m=\u001b[39m get_cfg_from_args(args)\n\u001b[1;32m     69\u001b[0m os\u001b[39m.\u001b[39mmakedirs(args\u001b[39m.\u001b[39moutput_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 70\u001b[0m default_setup(args) \n\u001b[1;32m     71\u001b[0m apply_scaling_rules_to_cfg(cfg)\n\u001b[1;32m     72\u001b[0m write_config(cfg, args\u001b[39m.\u001b[39moutput_dir)\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/utils/config.py:51\u001b[0m, in \u001b[0;36mdefault_setup\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_setup\u001b[39m(args):\n\u001b[0;32m---> 51\u001b[0m     distributed\u001b[39m.\u001b[39;49menable(overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     52\u001b[0m     seed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(args, \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     53\u001b[0m     rank \u001b[39m=\u001b[39m distributed\u001b[39m.\u001b[39mget_global_rank()\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/distributed/__init__.py:251\u001b[0m, in \u001b[0;36menable\u001b[0;34m(set_cuda_current_device, overwrite, allow_nccl_timeout)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mglobal\u001b[39;00m _LOCAL_RANK, _LOCAL_WORLD_SIZE\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m _LOCAL_RANK \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m _LOCAL_WORLD_SIZE \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDistributed mode has already been enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m torch_env \u001b[39m=\u001b[39m _TorchDistributedEnvironment()\n\u001b[1;32m    253\u001b[0m torch_env\u001b[39m.\u001b[39mexport(overwrite\u001b[39m=\u001b[39moverwrite)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Distributed mode has already been enabled"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='BTCV:split=TRAIN:root=/mnt/z/data/Abdomen/RawData', val_dataset_str='BTCV:split=VAL:root=/mnt/z/data/Abdomen/RawData', test_dataset_str='BTCV:split=TEST:root=/mnt/z/data/Abdomen/RawData', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "# feature_model = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=True)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_str = args.train_dataset_str\n",
    "val_dataset_str = args.val_dataset_str\n",
    "batch_size = args.batch_size\n",
    "gather_on_cpu = args.gather_on_cpu\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230924 12:41:42 467 dinov2 loaders.py:96] using dataset: \"BTCV:split=TRAIN:root=/mnt/z/data/Abdomen/RawData\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 scans are missing from TRAIN set\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:101] # of dataset samples: 15\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:96] using dataset: \"BTCV:split=VAL:root=/mnt/z/data/Abdomen/RawData\"\n",
      "0 scans are missing from VAL set\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:101] # of dataset samples: 15\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:96] using dataset: \"BTCV:split=TEST:root=/mnt/z/data/Abdomen/RawData\"\n",
      "0 scans are missing from TEST set\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:101] # of dataset samples: 20\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:124] sampler: infinite\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:218] using PyTorch data loader\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:233] infinite data loader\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:124] sampler: infinite\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:218] using PyTorch data loader\n",
      "I20230924 12:41:42 467 dinov2 loaders.py:233] infinite data loader\n"
     ]
    }
   ],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms()\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms()\n",
    "# train_image_transform = make_classification_train_transform()\n",
    "# eval_image_transform = make_classification_eval_transform()\n",
    "# train_target_transform = eval_target_transform = None\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=args.train_dataset_str, val_dataset_str=args.val_dataset_str,\n",
    "                                                        test_dataset_str=args.test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "\n",
    "sampler_type = SamplerType.INFINITE\n",
    "\n",
    "is_3d = test_dataset.is_3d()\n",
    "\n",
    "train_data_loader = make_data_loader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=sampler_type,\n",
    "    sampler_advance=0,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=collate_fn_3d if is_3d else None\n",
    ")\n",
    "\n",
    "val_data_loader = make_data_loader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=sampler_type,\n",
    "    sampler_advance=0,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=collate_fn_3d if is_3d else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('nibabel').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_dataset.get_image_data(0)\n",
    "lbl = train_dataset.get_target(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOV2Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, autocast_ctx, is_3d=False) -> None:\n",
    "        super(DINOV2Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "        self.is_3d = is_3d\n",
    "    \n",
    "    def forward_3d(self, x):\n",
    "        batch_features = [] \n",
    "        for batch_scans in x: # calculate the features for every scan in all scans of the batch\n",
    "            scans = []\n",
    "            for scan in batch_scans:\n",
    "                if not is_zero_matrix(scan): scans.append(self.forward_(scan.unsqueeze(0)))\n",
    "            batch_features.append(scans)\n",
    "        return batch_features\n",
    "\n",
    "    def forward_(self, x):\n",
    "        with torch.no_grad():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.encoder.forward_features(x)['x_norm_patchtokens']\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if is_3d:\n",
    "            return self.forward_3d(x)\n",
    "        return self.forward_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LinearDecoder(torch.nn.Module):\n",
    "#     \"\"\"Linear decoder head\"\"\"\n",
    "#     DECODER_TYPE = \"linear\"\n",
    "\n",
    "#     def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3, is_3d=False):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.in_channels = in_channels\n",
    "#         self.width = tokenW\n",
    "#         self.height = tokenH\n",
    "#         self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "#         self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         self.is_3d = is_3d\n",
    "\n",
    "#     def forward_3d(self, embeddings, vectorized=False):\n",
    "#         batch_outputs = []\n",
    "#         for batch_embeddings in embeddings:\n",
    "#             if vectorized:\n",
    "#                 batch_outputs.append(self.forward_(torch.stack(batch_embeddings).squeeze()))\n",
    "#             else:\n",
    "#                 batch_outputs.append(\n",
    "#                     torch.stack([self.forward_(slice_embedding) for slice_embedding in batch_embeddings]).squeeze()\n",
    "#                     )\n",
    "#         return torch.stack(batch_outputs)\n",
    "\n",
    "#     def forward_(self, embeddings):\n",
    "#         embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "#         embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "#         return self.decoder(embeddings)\n",
    "    \n",
    "#     def forward(self, embeddings):\n",
    "#         if self.is_3d:\n",
    "#             return self.forward_3d(embeddings)\n",
    "#         return self.forward_(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m features \u001b[39m=\u001b[39m f(i)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m output \u001b[39m=\u001b[39m ld(features)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39;49mshape)    \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(t, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "optimizer = torch.optim.SGD(ld.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda(non_blocking=True) \n",
    "\n",
    "    features = f(i)\n",
    "    output = ld(features)\n",
    "    \n",
    "    output = torch.cat(output, dim=0)\n",
    "    t = torch.cat(t, dim=0)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(output, t.cuda(non_blocking=True).type(torch.int64))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    optimizer.step()\n",
    "    # labels = t.view(-1, t.shape[-1], t.shape[-1])\n",
    "    # losses = nn.CrossEntropyLoss()(output.view(-1, 14, labels.shape[-1], labels.shape[-1]), labels)\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ld = LinearDecoder(in_channels=embed_dim, num_classes=3, is_3d=True)\n",
    "ld = ld.cuda()\n",
    "\n",
    "o = ld(features)\n",
    "print(len(o))\n",
    "print(o.shape)\n",
    "o = torch.stack([torch.nn.functional.interpolate(batch_output, size=448, mode=\"bilinear\", align_corners=False)\n",
    "                for batch_output in torch.unbind(o, dim=0)], dim=0)\n",
    "# ou = torch.nn.functional.interpolate(o[0], size=448, mode=\"bilinear\", align_corners=False)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
