{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_zero_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230921 11:42:15 22157 dinov2 config.py:60] git:\n",
      "  sha: c19d50789976d4bd62b2551d3b48f90fb84f81d2, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20230921 11:42:15 22157 dinov2 config.py:61] batch_size: 8\n",
      "comment: \n",
      "config_file: dinov2/configs/eval/vits14_pretrain.yaml\n",
      "exclude: \n",
      "gather_on_cpu: False\n",
      "n_per_class_list: [-1]\n",
      "n_tries: 1\n",
      "nb_knn: [5, 20, 50, 100, 200]\n",
      "ngpus: 1\n",
      "nodes: 1\n",
      "opts: ['train.output_dir=/mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn']\n",
      "output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "partition: learnlab\n",
      "pretrained_weights: models/dinov2_vits14_pretrain.pth\n",
      "temperature: 0.07\n",
      "test_dataset_str: NIHChestXray:split=TEST:root=/mnt/d/data/NIH\n",
      "timeout: 2800\n",
      "train_dataset_str: NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH\n",
      "use_volta32: False\n",
      "val_dataset_str: NIHChestXray:split=VAL:root=/mnt/d/data/NIH\n",
      "I20230921 11:42:15 22157 dinov2 config.py:27] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20230921 11:42:15 22157 dinov2 config.py:34] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: ImageNet:split=TRAIN\n",
      "  output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_small\n",
      "  patch_size: 14\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 518\n",
      "  local_crops_size: 98\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "I20230921 11:42:15 22157 dinov2 vision_transformer.py:110] using MLP layer as FFN\n",
      "I20230921 11:42:16 22157 dinov2 utils.py:35] Pretrained weights found at models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/mnt/d/data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "feature_model = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=True)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        split_lengths = {\n",
    "            _Split.TRAIN: 15,\n",
    "            _Split.VAL: 15,\n",
    "            _Split.TEST: 20,\n",
    "        }\n",
    "        return split_lengths[self]\n",
    "\n",
    "class BTCV(MedicalVisionDataset):\n",
    "    Split = _Split\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        split: \"Shenzhen.Split\",\n",
    "        root: str,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(split, root, transforms, transform, target_transform)\n",
    "\n",
    "        self._image_path = self._split_dir + os.sep + \"img\"\n",
    "        self.images = os.listdir(self._image_path)\n",
    "\n",
    "        if self._split != _Split.TEST:\n",
    "            self._masks_path = self._split_dir + os.sep + \"label\"\n",
    "    \n",
    "        self.class_id_mapping = {    \n",
    "            0: \"background\",\n",
    "            1: \"spleen\",\n",
    "            2: \"rkid\",\n",
    "            3: \"lkid\",\n",
    "            4: \"gall\",\n",
    "            5: \"eso\",\n",
    "            6: \"liver\",\n",
    "            7: \"sto\",\n",
    "            8: \"aorta\",\n",
    "            9: \"IVC\",\n",
    "            10: \"veins\",\n",
    "            11: \"pancreas\",\n",
    "            12: \"rad\",\n",
    "            13: \"lad\"\n",
    "        }\n",
    "        self.class_names = list(self.class_id_mapping.keys())\n",
    "\n",
    "    def _check_size(self):\n",
    "        num_of_images = len(os.listdir(self._split_dir + os.sep + \"img\"))\n",
    "        print(f\"{self._split.length - num_of_images} scans are missing from {self._split.value.upper()} set\")\n",
    "\n",
    "    def get_num_classes(self) -> int:\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def is_3d(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def get_image_data(self, index: int) -> np.ndarray:\n",
    "        image_folder_path = self._image_path + os.sep + self.images[index]\n",
    "        image_path = image_folder_path + os.sep + os.listdir(image_folder_path)[0]  \n",
    "\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        image = np.stack((image,)*3, axis=0)\n",
    "        print(image.shape)\n",
    "        image = torch.from_numpy(image).permute(3, 0, 1, 2).float()\n",
    "\n",
    "        return image\n",
    "    \n",
    "    # def get_target(self, index: int) -> np.ndarray:\n",
    "\n",
    "\n",
    "    #     return target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image = self.get_image_data(index)\n",
    "        target = self.get_target(index)\n",
    "\n",
    "        seed = np.random.randint(2147483647) # make a seed with numpy generator \n",
    "        if self.transform is not None:\n",
    "            np.random.seed(seed), torch.manual_seed(seed) \n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            np.random.seed(seed), torch.manual_seed(seed) \n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        # Remove channel dim in target\n",
    "        target = target.squeeze()\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 scans are missing from TRAIN set\n"
     ]
    }
   ],
   "source": [
    "d = BTCV(root=\"/mnt/z/data/Abdomen/RawData\", split=_Split.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230920 17:53:55 373 nibabel.global batteryrunners.py:268] pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "(3, 512, 512, 89)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([89, 3, 512, 512])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.get_image_data(8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/z/data/Abdomen/RawData/Training/img\"\n",
    "\n",
    "\n",
    "imgs = os.listdir(data_dir)\n",
    "one_path = data_dir + os.sep + imgs[1]\n",
    "one = os.listdir(one_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230920 16:59:24 373 nibabel.global batteryrunners.py:268] pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    }
   ],
   "source": [
    "image = nib.load(one_path + os.sep + one[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = image.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 139)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_str = args.train_dataset_str\n",
    "val_dataset_str = args.val_dataset_str\n",
    "batch_size = args.batch_size\n",
    "gather_on_cpu = args.gather_on_cpu\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        split_lengths = {\n",
    "            _Split.TRAIN: 90,\n",
    "            _Split.VAL: 50,\n",
    "            _Split.TEST: 70,\n",
    "        }\n",
    "        return split_lengths[self]\n",
    "\n",
    "class SARSCoV2CT(MedicalVisionDataset):\n",
    "    Split = _Split\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        split: \"SARSCoV2CT.Split\",\n",
    "        root: str,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(split, root, transforms, transform, target_transform)\n",
    "\n",
    "        self.class_names = [\n",
    "            \"Negative\",\n",
    "            \"Positive\"\n",
    "        ]\n",
    "        \n",
    "    @property\n",
    "    def split(self) -> \"SARSCoV2CT.Split\":\n",
    "        return self._split\n",
    "\n",
    "    def get_length(self) -> int:\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_num_classes(self) -> int:\n",
    "        return 2\n",
    "    \n",
    "    def is_3d(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def is_multilabel(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def get_image_data(self, index: int) -> np.ndarray:\n",
    "        scans_path = self._split_dir + os.sep + self.images[index]\n",
    "        scans = os.listdir(scans_path)\n",
    "        scans = [\".\".join(scan.split(\".\")[:-1]) for scan in scans]\n",
    "        \n",
    "        if scans[0].isnumeric():\n",
    "            scans = [int(scan) for scan in scans]\n",
    "            scans.sort()\n",
    "\n",
    "        for i, scan in enumerate(scans):\n",
    "            \n",
    "            scan = skimage.io.imread(scans_path + os.sep + str(scan) + \".png\")\n",
    "            scan = scan[:, :, :3]\n",
    "            scan = torch.from_numpy(scan).permute(2, 0, 1).float()\n",
    "\n",
    "            scans[i] = scan \n",
    "\n",
    "        return scans\n",
    "    \n",
    "    def get_target(self, index: int) -> int:\n",
    "        return int(int(self.images[index]) <= 79) # IDs 0-79 are positive\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        images = self.get_image_data(index)\n",
    "        target = self.get_target(index)\n",
    "\n",
    "        seed = np.random.randint(2147483647) # make a seed with numpy generator \n",
    "        if self.transforms is not None:\n",
    "            for i in range(len(images)):\n",
    "                np.random.seed(seed), torch.manual_seed(seed) \n",
    "                images[i] = self.transform(images[i])\n",
    "            images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, l = make_segmentation_train_transforms(resize_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m transform \u001b[39m=\u001b[39m target_transform \u001b[39m=\u001b[39m make_classification_train_transform()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m dataset \u001b[39m=\u001b[39m SARSCoV2CT(split\u001b[39m=\u001b[39mSARSCoV2CT\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                 root\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/mnt/z/data/SARS-CoV-2-CT\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                 transform\u001b[39m=\u001b[39ma)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "transform = target_transform = make_classification_train_transform()\n",
    "dataset = SARSCoV2CT(split=SARSCoV2CT.Split.TRAIN,\n",
    "                root=\"/mnt/z/data/SARS-CoV-2-CT\",\n",
    "                transform=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in dataset:\n",
    "    i.cuda()\n",
    "    show_image_from_tensor(i[0])\n",
    "    show_image_from_tensor(i[1])\n",
    "    show_image_from_tensor(i[2])\n",
    "    show_image_from_tensor(i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(torch.concat(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_data_loader \u001b[39m=\u001b[39m make_data_loader(\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     dataset\u001b[39m=\u001b[39mdataset,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39mcollate_fn_3d,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     sampler_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     sampler_advance\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     persistent_workers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_loader = make_data_loader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=collate_fn_3d,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=None,\n",
    "    sampler_advance=1,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "\n",
    "    def __init__(self, out_dim, use_n_blocks, use_avgpool, num_classes=1000, is_3d=False):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.use_n_blocks = use_n_blocks\n",
    "        self.use_avgpool = use_avgpool\n",
    "        self.num_classes = num_classes\n",
    "        self.linear = nn.Linear(out_dim, num_classes)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.is_3d = is_3d\n",
    "\n",
    "    def forward_3d(self, inputs):\n",
    "        outputs_per_batch = []\n",
    "        for batch in inputs:\n",
    "            outputs_per_batch.append(self.forward_(batch))\n",
    "        outputs = torch.stack(outputs_per_batch).squeeze()\n",
    "        return outputs\n",
    "    \n",
    "    def forward_(self, inputs):\n",
    "        output = torch.stack( # If 3D, take average of all slices.\n",
    "            [create_linear_input(image, self.use_n_blocks, self.use_avgpool) for image in inputs]\n",
    "            ).mean(dim=0)\n",
    "        return output.squeeze()\n",
    "    \n",
    "    def forward(self, images):\n",
    "        if self.is_3d: output = self.forward_3d(images)\n",
    "        else: output = self.forward_(images)\n",
    "\n",
    "        return self.linear(output).squeeze()\n",
    "\n",
    "lc = LinearClassifier(384, 1, False, 1, is_3d=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = feature_model.forward_(dataset[0][0][0].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_linear_classifiers(sample_output, n_last_blocks_list, learning_rates, avgpools=[True, False], num_classes=14, is_3d=False):\n",
    "    \"\"\"\n",
    "    Sets up the multiple linear classifiers with different hyperparameters to test out the most optimal one \n",
    "    \"\"\"\n",
    "    linear_classifiers_dict = nn.ModuleDict()\n",
    "    optim_param_groups = []\n",
    "    for n in n_last_blocks_list:\n",
    "        for avgpool in avgpools:\n",
    "            for _lr in learning_rates:\n",
    "                # lr = scale_lr(_lr, batch_size)\n",
    "                lr = _lr\n",
    "                out_dim = create_linear_input(sample_output, use_n_blocks=n, use_avgpool=avgpool).shape[1]\n",
    "                linear_classifier = LinearClassifier(\n",
    "                    out_dim, use_n_blocks=n, use_avgpool=avgpool, num_classes=num_classes, is_3d=is_3d\n",
    "                )\n",
    "                linear_classifier = linear_classifier.cuda()\n",
    "                linear_classifiers_dict[\n",
    "                    f\"linear:blocks={n}:avgpool={avgpool}:lr={lr:.10f}\".replace(\".\", \"_\")\n",
    "                ] = linear_classifier\n",
    "                optim_param_groups.append({\"params\": linear_classifier.parameters(), \"lr\": lr})\n",
    "\n",
    "    linear_classifiers = AllClassifiers(linear_classifiers_dict)\n",
    "    if distributed.is_enabled():\n",
    "        linear_classifiers = nn.parallel.DistributedDataParallel(linear_classifiers)\n",
    "\n",
    "    return linear_classifiers, optim_param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifiers, optim_param_groups = setup_linear_classifiers(\n",
    "    sample_output=sample_output,\n",
    "    n_last_blocks_list=[1, 4],\n",
    "    learning_rates=[1e-2, 1e-4],\n",
    "    avgpools=[False, True],\n",
    "    num_classes=1,\n",
    "    is_3d=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllClassifiers(nn.Module):\n",
    "    def __init__(self, classifiers_dict):\n",
    "        super().__init__()\n",
    "        self.classifiers_dict = nn.ModuleDict()\n",
    "        self.classifiers_dict.update(classifiers_dict)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print(inputs)\n",
    "        print(\"1\")\n",
    "        return {k: v.forward(inputs) for k, v in self.classifiers_dict.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.classifiers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0\n",
    "f = 0\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    t = t.cuda()\n",
    "\n",
    "    features = feature_model(i)  # batch then slices\n",
    "    # output = lc(features)\n",
    "    output = linear_classifiers(features)\n",
    "    break\n",
    "\n",
    "    # outputs = [\n",
    "    #     list(self.all_classifiers_forward(batch_feature).values()) for batch_feature in inputs\n",
    "    #     ]\n",
    "    # classifier_outputs = [torch.stack(output).squeeze() for output in outputs] # stack across classifiers\n",
    "    # outputs = torch.stack(classifier_outputs, dim=1) # stack across batch\n",
    "    # classifiers = list(self.classifiers_dict.keys())\n",
    "    # outputs = { # output for every classifer\n",
    "    #     classifiers[i]: output \n",
    "    #     for i, output in enumerate(outputs)\n",
    "    # }\n",
    "    # return outputs\n",
    "    break\n",
    "    outputs = [lc(batch_feature) for batch_feature in batch_features]\n",
    "    outputs = torch.stack(outputs, dim=0).squeeze(1)\n",
    "    print(outputs)\n",
    "    break\n",
    "    if z == 5:\n",
    "        break\n",
    "    z+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleImage:\n",
    "    def __call__(self, image):\n",
    "        if isinstance(image, np.ndarray):\n",
    "            # Convert to tensor\n",
    "            image = torch.from_numpy(image)\n",
    "        elif torch.is_tensor(image):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\"Input should be of type numpy.ndarray or torch.Tensor\")\n",
    "\n",
    "        # Rescale the tensor to [0, 1]\n",
    "        min_val = image.min()\n",
    "        max_val = image.max()\n",
    "        return (image - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "class MaybeToTensor(transforms.PILToTensor):\n",
    "    \"\"\"\n",
    "    Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor, or keep as is if already a tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL Image, numpy.ndarray or torch.tensor): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        if isinstance(pic, torch.Tensor):\n",
    "            return pic\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            return torch.from_numpy(pic)\n",
    "        return super().__call__(pic)\n",
    "\n",
    "# Use timm's names\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def make_normalize_transform(\n",
    "    mean= IMAGENET_DEFAULT_MEAN,\n",
    "    std  = IMAGENET_DEFAULT_STD,\n",
    ") -> transforms.Normalize:\n",
    "    return transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "\n",
    "def make_segmentation_train_transforms(\n",
    "    *,\n",
    "    resize_size: int = 448,\n",
    "    vflip_prob: float = 0.5,\n",
    "    rot_deg: float = 45,\n",
    "    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    mean= IMAGENET_DEFAULT_MEAN,\n",
    "    std = IMAGENET_DEFAULT_STD,\n",
    ") -> transforms.Compose:\n",
    "    train_transforms_list = [\n",
    "        transforms.Resize((resize_size, resize_size), interpolation=interpolation)\n",
    "        ]\n",
    "    target_transforms_list = [\n",
    "        transforms.Resize((resize_size, resize_size), interpolation=interpolation)\n",
    "        ]\n",
    "    if vflip_prob > 0:\n",
    "        train_transforms_list.append(transforms.RandomVerticalFlip(vflip_prob))\n",
    "        target_transforms_list.append(transforms.RandomVerticalFlip(vflip_prob))\n",
    "    if rot_deg > 0:\n",
    "        train_transforms_list.append(transforms.RandomRotation(rot_deg))\n",
    "        target_transforms_list.append(transforms.RandomRotation(rot_deg))\n",
    "\n",
    "    train_transforms_list.extend([    \n",
    "        MaybeToTensor(),\n",
    "        RescaleImage(),\n",
    "        # make_normalize_transform(mean=mean, std=std),\n",
    "    ])\n",
    "    target_transforms_list.append(MaybeToTensor())\n",
    "\n",
    "    return (transforms.Compose(train_transforms_list),\n",
    "            transforms.Compose(target_transforms_list))\n",
    "\n",
    "def make_classification_train_transform(\n",
    "    *,\n",
    "    crop_size: int = 224,\n",
    "    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    hflip_prob: float = 0.5,\n",
    "    mean = IMAGENET_DEFAULT_MEAN,\n",
    "    std = IMAGENET_DEFAULT_STD,\n",
    "):\n",
    "    transforms_list = [\n",
    "        transforms.RandomResizedCrop(crop_size, scale=(0.75, 1), interpolation=interpolation),\n",
    "    ]\n",
    "    if hflip_prob > 0.0:\n",
    "        transforms_list.append(transforms.RandomHorizontalFlip(hflip_prob))\n",
    "    transforms_list.extend(\n",
    "        [\n",
    "            MaybeToTensor(),\n",
    "            RescaleImage(),\n",
    "            # make_normalize_transform(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230921 11:44:08 22157 dinov2 loaders.py:94] using dataset: \"NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH\"\n",
      "I20230921 11:44:08 22157 dinov2 medical_dataset.py:36] 0 scans are missing from TRAIN set\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:99] # of dataset samples: 76,522\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:94] using dataset: \"NIHChestXray:split=VAL:root=/mnt/d/data/NIH\"\n",
      "I20230921 11:44:09 22157 dinov2 medical_dataset.py:36] 0 scans are missing from VAL set\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:99] # of dataset samples: 10,002\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:94] using dataset: \"NIHChestXray:split=TEST:root=/mnt/d/data/NIH\"\n",
      "I20230921 11:44:09 22157 dinov2 medical_dataset.py:36] 0 scans are missing from TEST set\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:99] # of dataset samples: 25,596\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:122] sampler: infinite\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:216] using PyTorch data loader\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:231] infinite data loader\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:122] sampler: infinite\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:216] using PyTorch data loader\n",
      "I20230921 11:44:09 22157 dinov2 loaders.py:231] infinite data loader\n"
     ]
    }
   ],
   "source": [
    "# train_image_transform, train_target_transform = make_segmentation_train_transforms()\n",
    "# eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms()\n",
    "train_image_transform = make_classification_train_transform()\n",
    "eval_image_transform = make_classification_eval_transform()\n",
    "train_target_transform = eval_target_transform = None\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=args.train_dataset_str, val_dataset_str=args.val_dataset_str,\n",
    "                                                        test_dataset_str=args.test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "\n",
    "sampler_type = SamplerType.INFINITE\n",
    "\n",
    "train_data_loader = make_data_loader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=sampler_type,\n",
    "    sampler_advance=1,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "val_data_loader = make_data_loader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=sampler_type,\n",
    "    sampler_advance=1,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "tensor(1., dtype=torch.float16)\n",
      "tensor(0., dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCADgAOABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiv/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAAA+klEQVR4Ae3QMQ0AAAzDsI4/6VHo2cNBEDkRAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECOwK3O6aMwIECBCoBB7KqgACKLdkywAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mmin())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m show_image_from_tensor(i)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m show_image_from_tensor(t\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "for i, t in train_dataset:\n",
    "    i.cuda() \n",
    "    # t.cuda()\n",
    "    \n",
    "    print(i)\n",
    "\n",
    "    print(i.shape)\n",
    "\n",
    "    print(i.max())\n",
    "    print(i.min())\n",
    "\n",
    "    show_image_from_tensor(i)\n",
    "    show_image_from_tensor(t.unsqueeze(0) * 100)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
