{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "from typing import Any, Dict, Optional\n",
    "from collections import OrderedDict\n",
    "from monai.losses.dice import DiceLoss, DiceCELoss\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.wrappers import ClasswiseWrapper\n",
    "from torchmetrics.classification import (MultilabelAUROC, MultilabelF1Score, MultilabelAccuracy, MulticlassF1Score, \n",
    "                                        MulticlassAccuracy, MulticlassAUROC, Accuracy, BinaryF1Score, BinaryAUROC,\n",
    "                                        JaccardIndex, MulticlassJaccardIndex, Dice, BinaryAUROC)\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT, BTCV, BTCVSlice, AMOS, MSDHeart\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_padded_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets, make_data_loaders)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics, MetricAveraging, MetricType\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemicSampler(torch.utils.data.Subset):\n",
    "    def __init__(self, dataset, num_samples=5000):\n",
    "        dataset_len = dataset.__len__()\n",
    "        indices = np.arange(0, dataset_len, dataset_len/num_samples).round().astype(\"int\")\n",
    "        self.subset = torch.utils.data.Subset(dataset, indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.subset.__getitem__(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.subset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "os.environ['TMPDIR'] = 'D:/data/tmp'\n",
    "\n",
    "def resize_and_save_image(image_path, output_dir):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Resize the image\n",
    "        img = img.resize((1024, 1024))\n",
    "        # Create the same directory structure in the output directory\n",
    "        output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "        # os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        # Save the image\n",
    "        print(output_path)\n",
    "        # img.save(output_path)\n",
    "\n",
    "def process_dataset(input_dir, output_dir):\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                # Get the full file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Resize and save the image\n",
    "                resize_and_save_image(file_path, output_dir)\n",
    "\n",
    "# Call the function\n",
    "process_dataset('D:/data/cheXpert/CheXpert-v1.0', 'D:/data/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/z/data/amos22/\"\n",
    "train_data_path = data_dir + \"train/\"\n",
    "train_data = os.listdir(train_data_path)\n",
    "\n",
    "val_data_path = data_dir + \"val/\"\n",
    "val_data = os.listdir(val_data_path)\n",
    "\n",
    "test_data_path = data_dir + \"test/\"\n",
    "test_data = os.listdir(test_data_path)\n",
    "\n",
    "label_data_path = data_dir + \"labels/\"\n",
    "label_data = os.listdir(label_data_path)\n",
    "\n",
    "for label in label_data:\n",
    "    if label not in train_data or label not in val_data or label not in test_data:\n",
    "        os.remove(label_data_path + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/z/data/amos22/\"\n",
    "images_data_path = data_dir + \"images/\"\n",
    "images_data = os.listdir(images_data_path)\n",
    "\n",
    "train_data_path = data_dir + \"train\"\n",
    "val_data_path = data_dir + \"val\"\n",
    "test_data_path = data_dir + \"test\"\n",
    "label_data_path = data_dir + \"labels\"\n",
    "\n",
    "# os.makedirs(train_data_path, exist_ok=True)\n",
    "# train_images = np.arange(0, 300, 300/80).round().astype(\"int\")\n",
    "# os.makedirs(val_data_path, exist_ok=True)\n",
    "# val_images = np.arange(0, 220, 220/20).round().astype(\"int\")\n",
    "# os.makedirs(test_data_path, exist_ok=True)\n",
    "# test_images = np.arange(0, 200, 200/50).round().astype(\"int\")\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in train_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, train_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in val_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, val_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in test_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, test_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "train_data = os.listdir(train_data_path)\n",
    "val_data = os.listdir(val_data_path)\n",
    "test_data = os.listdir(test_data_path)\n",
    "label_data = os.listdir(label_data_path)\n",
    "\n",
    "for scan in label_data:\n",
    "    if scan not in train_data and scan not in val_data and scan not in test_data:\n",
    "        os.remove(label_data_path + os.sep + scan)\n",
    "\n",
    "for scan in train_data:\n",
    "    nifit_scan = nib.load(train_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    scan_name = scan.split(\".\")[0]\n",
    "\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(train_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(train_data_path + os.sep + scan)\n",
    "\n",
    "for scan in val_data:\n",
    "    nifit_scan = nib.load(val_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    \n",
    "    scan_name = scan.split(\".\")[0]\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(val_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(val_data_path + os.sep + scan)\n",
    "\n",
    "for scan in test_data:\n",
    "    nifit_scan = nib.load(test_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    \n",
    "    scan_name = scan.split(\".\")[0]\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(test_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(test_data_path + os.sep + scan)\n",
    "\n",
    "for scan in label_data:\n",
    "    nifit_scan = nib.load(label_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    scan_name = scan.split(\".\")[0]\n",
    "\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(label_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(label_data_path + os.sep + scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import SwinUNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/mnt/d/data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='MC:split=TRAIN:root=/mnt/z/data/MC', val_dataset_str='MC:split=VAL:root=/mnt/z/data/MC', test_dataset_str='MC:split=TEST:root=/mnt/z/data/MC', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# args = argparse.Namespace(backbone=\"dinov2\", config_file='dinov2/configs/eval/vitl14_pretrain.yaml', pretrained_weights='models/dinov2_vitl14_pretrain.pth', output_dir='results/NIH/dinov2_vitl14/knn', opts=[], train_dataset_str='BTCV:split=TRAIN:root=/mnt/z/data/BTCV', test_dataset_str='BTCV:split=VAL:root=/mnt/z/data/BTCV', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# model, autocast_dtype = setup_and_build_model(args)\n",
    "# autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 4, autocast_ctx, is_3d=False)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.eval.segmentation.utils import UNetDecoder\n",
    "m = UNetDecoder(1536, out_channels=1, image_size=448)\n",
    "decoder = LinearDecoder(\n",
    "    1024, num_classes=1, image_size=448, patch_size=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier = LinearClassifier(\n",
    "    1536, use_n_blocks=1, use_avgpool=False, num_classes=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "ViTForImageClassification.from_pretrained('facebook/vit-mae-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "ViTForImageClassification.from_pretrained('openai/clip-vit-large-patch14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.eval.utils import trainable_parameters\n",
    "tp, ap = trainable_parameters(model)\n",
    "print(f\"LoRA trainable params: {tp} || all params: {ap} || trainable%: {100 * tp / ap:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "from transformers import (\n",
    "    SamVisionConfig,\n",
    "    SamPromptEncoderConfig,\n",
    "    SamMaskDecoderConfig,\n",
    "    SamModel,\n",
    "    SamConfig,\n",
    ")\n",
    "\n",
    "# config = SamConfig(SamVisionConfig(image_size=1024))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.rand([1, 3, 1024, 1024]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model.get_image_embeddings(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.eval.utils import bitfit\n",
    "model = bitfit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_total_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "images = BTCVSlice(root=\"/mnt/z/data/BTCVSlice/\", split=_Split.TRAIN, transform=train_image_transform, target_transform=train_target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    print(i.shape)\n",
    "    l = model.get_image_embeddings(i[0].unsqueeze(0))\n",
    "    print(l.shape)\n",
    "    print(np.unique(t))\n",
    "    print(t.max())\n",
    "    show_image_from_tensor(t[6].unsqueeze(0) * 50 )\n",
    "    print(images.get_num_classes())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "# eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "train_image_transform = make_classification_train_transform()\n",
    "eval_image_transform = make_classification_eval_transform()\n",
    "\n",
    "train_target_transform = eval_target_transform = None\n",
    "\n",
    "val_dataset_str = args.val_dataset_str\n",
    "# val_dataset_str = None\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=args.train_dataset_str, val_dataset_str=val_dataset_str,\n",
    "                                                        test_dataset_str=args.test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "is_3d = test_dataset.is_3d()\n",
    "collate_fn=collate_fn_3d if is_3d else None\n",
    "start_iter=1\n",
    "sampler_type = SamplerType.INFINITE\n",
    "# sampler_type = None\n",
    "seed = 0\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "train_data_loader, val_data_loader, test_data_loader = make_data_loaders(train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                                                                        val_dataset=val_dataset, sampler_type=sampler_type, seed=seed,\n",
    "                                                                        start_iter=start_iter, batch_size=batch_size, num_workers=num_workers,\n",
    "                                                                        collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=train_dataset,\n",
    "#     batch_size=2,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=images,\n",
    "#     batch_size=4,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "# val_data_loader = make_data_loader(\n",
    "#     dataset=val_dataset,\n",
    "#     batch_size=4,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=None,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = SystemicSampler(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 1 1 0 0 0 1 0 0 0 1 1 0]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 1 1 0 0 1 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m a:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(t)\n",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 29\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubset\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/data/datasets/nih_chest_xray.py:117\u001b[0m, in \u001b[0;36mNIHChestXray.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 117\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_image_data(index)\n\u001b[1;32m    118\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_target(index)\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/data/datasets/nih_chest_xray.py:104\u001b[0m, in \u001b[0;36mNIHChestXray.get_image_data\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    101\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_dir \u001b[39m+\u001b[39m os\u001b[39m.\u001b[39msep \u001b[39m+\u001b[39m data_point[\u001b[39m\"\u001b[39m\u001b[39mImage Index\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[39m# Read as gray because some of the images have extra layers in the 3rd dimension\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m image \u001b[39m=\u001b[39m skimage\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mimread(image_path, as_gray\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat16)\n\u001b[1;32m    105\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack((image,)\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    106\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(image)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skimage/io/_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[0;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39;49m\u001b[39mimread\u001b[39;49m\u001b[39m'\u001b[39;49m, fname, plugin\u001b[39m=\u001b[39;49mplugin, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mplugin_args)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skimage/io/manage_plugins.py:205\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mplugin\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m{\u001b[39;00mkind\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skimage/io/_plugins/imageio_plugin.py:11\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 11\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mflags[\u001b[39m'\u001b[39m\u001b[39mWRITEABLE\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     13\u001b[0m         out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imageio/v3.py:54\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m index\n\u001b[1;32m     53\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mplugin_kwargs) \u001b[39mas\u001b[39;00m img_file:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(img_file\u001b[39m.\u001b[39;49mread(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcall_kwargs))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imageio/plugins/pillow.py:224\u001b[0m, in \u001b[0;36mPillowPlugin.read\u001b[0;34m(self, index, mode, rotate, apply_gamma, writeable_output, pilmode, exifrotate, as_gray)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, \u001b[39mint\u001b[39m):\n\u001b[1;32m    222\u001b[0m     \u001b[39m# will raise IO error if index >= number of frames in image\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image\u001b[39m.\u001b[39mseek(index)\n\u001b[0;32m--> 224\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_transforms(\n\u001b[1;32m    225\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_image, mode, rotate, apply_gamma, writeable_output\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter(\n\u001b[1;32m    229\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    230\u001b[0m         rotate\u001b[39m=\u001b[39mrotate,\n\u001b[1;32m    231\u001b[0m         apply_gamma\u001b[39m=\u001b[39mapply_gamma,\n\u001b[1;32m    232\u001b[0m         writeable_output\u001b[39m=\u001b[39mwriteable_output,\n\u001b[1;32m    233\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imageio/plugins/pillow.py:307\u001b[0m, in \u001b[0;36mPillowPlugin._apply_transforms\u001b[0;34m(self, image, mode, rotate, apply_gamma, writeable_output)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    304\u001b[0m         \u001b[39m# Let pillow know that it is okay to return 16-bit\u001b[39;00m\n\u001b[1;32m    305\u001b[0m         image\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m desired_mode\n\u001b[0;32m--> 307\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(image)\n\u001b[1;32m    309\u001b[0m meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata(index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image\u001b[39m.\u001b[39mtell(), exclude_applied\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    310\u001b[0m \u001b[39mif\u001b[39;00m rotate \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mOrientation\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m meta:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:673\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtobytes(\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    672\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 673\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtobytes()\n\u001b[1;32m    674\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (\u001b[39mMemoryError\u001b[39;00m, \u001b[39mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:732\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m encoder_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m args \u001b[39m==\u001b[39m ():\n\u001b[1;32m    730\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[0;32m--> 732\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    735\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/ImageFile.py:249\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 249\u001b[0m         s \u001b[39m=\u001b[39m read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecodermaxblock)\n\u001b[1;32m    250\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, struct\u001b[39m.\u001b[39merror) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    251\u001b[0m         \u001b[39m# truncated png/gif\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[39mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/PngImagePlugin.py:952\u001b[0m, in \u001b[0;36mPngImageFile.load_read\u001b[0;34m(self, read_bytes)\u001b[0m\n\u001b[1;32m    948\u001b[0m     read_bytes \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(read_bytes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__idat)\n\u001b[1;32m    950\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__idat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__idat \u001b[39m-\u001b[39m read_bytes\n\u001b[0;32m--> 952\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(read_bytes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, t in a:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3DWrapper(nn.Module):\n",
    "    def __init__(self, model, per_slice=False) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.per_slice = per_slice\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_outputs = []\n",
    "        for slices in x: \n",
    "            if self.per_slice:\n",
    "                batch_outputs.append(\n",
    "                    torch.stack([self.model(slice_) for slice_ in slices], dim=0).squeeze()\n",
    "                )\n",
    "            else:\n",
    "                batch_outputs.append(\n",
    "                    self.model(slices)\n",
    "                )\n",
    "        return batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset.class_names\n",
    "num_of_classes = train_dataset.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build_segmentation_metrics(MetricType.SEGMENTATION_METRICS.accuracy_averaging, num_labels=7, labels=labels.tolist())\n",
    "m = m.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, lbl1 = test_dataset[0]\n",
    "slice1, slice1lbl = img1[130].cuda(non_blocking=True), lbl1[130].cuda(non_blocking=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = slice1.unsqueeze(0)\n",
    "slice1lbl = slice1lbl.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = slice1.type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=False).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=7, image_size=224).cuda()\n",
    "# decoder = Model3DWrapper(decoder, per_slice=True)\n",
    "optimizer = torch.optim.SGD(decoder.parameters(), lr=3.5e-4, momentum=0.9, weight_decay=0)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "for i in range(1000):\n",
    "    o1 = feature_model(slice1)\n",
    "    o2 = decoder(o1)\n",
    "\n",
    "    \n",
    "    o2 = o2.type(torch.float16)\n",
    "    print(slice1lbl.dtype)\n",
    "    loss = loss_function(o2, slice1lbl)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        # o2 = torch.argmax(o2, dim=1)\n",
    "        print(loss)\n",
    "        # print(m(o2, slice1lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.logging import MetricLogger\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = Model3DWrapper(LinearDecoder(in_channels=model.embed_dim, num_classes=14, image_size=224).cuda(), per_slice=True)\n",
    "        \n",
    "for samples, targets in train_data_loader:\n",
    "\n",
    "    samples = samples.cuda(non_blocking=True)\n",
    "\n",
    "    o1 = feature_model(samples)\n",
    "    # o2 = decoder(o1)\n",
    "\n",
    "    # o2 = torch.cat(o2, dim=0).cuda()\n",
    "    # targets = torch.cat(targets, dim=0).cuda()\n",
    "    # preds = o2.argmax(dim=1)\n",
    "    # targets = targets.type(torch.int64)\n",
    "\n",
    "    # print(o2.shape)\n",
    "    # print(targets.shape)\n",
    "\n",
    "    # z = {\n",
    "    #     \"preds\": preds[50:80],\n",
    "    #     \"target\": targets[50:80],\n",
    "    # }\n",
    "\n",
    "    # print(m.update(**z))\n",
    "    # print(m)\n",
    "    # print(m.compute())\n",
    "\n",
    "    break \n",
    "    # print(t.shape)\n",
    "    # print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoderUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, embed_dim=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*2, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.skip_conv = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upconv(x1)\n",
    "        x2 = self.skip_conv(x2)\n",
    "        scale_factor = (x1.size()[2] / x2.size()[2])\n",
    "        x2 = nn.Upsample(scale_factor=scale_factor, mode=\"bilinear\", align_corners=True)(x2)\n",
    "        x = torch.concat([x1, x2], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, image_size=224):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.embed_dim = in_channels\n",
    "        self.image_size = image_size\n",
    "        self.out_channels = out_channels\n",
    "        self.up1 = UNetDecoderUpBlock(in_channels=in_channels, out_channels=in_channels//2, embed_dim=embed_dim)\n",
    "        self.up2 = UNetDecoderUpBlock(in_channels=in_channels//2, out_channels=in_channels//4, embed_dim=embed_dim)\n",
    "        self.up3 = UNetDecoderUpBlock(in_channels=in_channels//4, out_channels=in_channels//8, embed_dim=embed_dim)\n",
    "        self.up4 = UNetDecoderUpBlock(in_channels=in_channels//8, out_channels=out_channels, embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = w = self.image_size//14\n",
    "\n",
    "        skip1 = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip2 = x[2].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip3 = x[1].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip4 = x[0].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        x1    = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        \n",
    "        x2 = self.up1(x1, skip1)\n",
    "        x3 = self.up2(x2, skip2)\n",
    "        x4 = self.up3(x3, skip3)\n",
    "        x5 = self.up4(x4, skip4)\n",
    "\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    print(i.shape)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True, image_size=224).cuda()\n",
    "# feature_model.eval()\n",
    "# decoder = UNetDecoder(in_channels=model.embed_dim, out_channels=3).cuda()\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=False)\n",
    "# feature_model_with_inter.eval()\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    print(i.shape)\n",
    "    embeddings = feature_model(i)\n",
    "    output = decoder(embeddings)\n",
    "    output = torch.stack(output, dim=0)\n",
    "    t = torch.stack(t, dim=0)\n",
    "    # print(t.unsqueeze(1).shape)\n",
    "    print(output.shape)\n",
    "    print(t.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_dataset.get_image_data(0)\n",
    "lbl = test_dataset.get_target(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOV2Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, autocast_ctx, is_3d=False) -> None:\n",
    "        super(DINOV2Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "        self.is_3d = is_3d\n",
    "    \n",
    "    def forward_3d(self, x):\n",
    "        batch_features = [] \n",
    "        for batch_scans in x: # calculate the features for every scan in all scans of the batch\n",
    "            scans = []\n",
    "            for scan in batch_scans:\n",
    "                if not is_zero_matrix(scan): scans.append(self.forward_(scan.unsqueeze(0)))\n",
    "            batch_features.append(scans)\n",
    "        return batch_features\n",
    "\n",
    "    def forward_(self, x):\n",
    "        with torch.no_grad():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.encoder.forward_features(x)['x_norm_patchtokens']\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if is_3d:\n",
    "            return self.forward_3d(x)\n",
    "        return self.forward_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in test_dataset:\n",
    "    show_image_from_tensor(i[0] * 100)\n",
    "    show_image_from_tensor(i[1] * 100)\n",
    "    show_image_from_tensor(i[2] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_results(feature_model, decoder, dataset):\n",
    "    for i, (img, _) in enumerate(dataset):\n",
    "\n",
    "        img_name = test_dataset.images[i]\n",
    "        _, affine_matrix = test_dataset.get_image_data(i, return_affine_matrix=True)\n",
    "\n",
    "        img = img.cuda(non_blocking=True) \n",
    "\n",
    "        features = feature_model(img.unsqueeze(0))\n",
    "        output = decoder(features, up_size=512)[0]\n",
    "        output = output.argmax(dim=1)\n",
    "\n",
    "        nifti_img = nib.Nifti1Image(output\n",
    "                                    .cpu()\n",
    "                                    .numpy()\n",
    "                                    .astype(np.uint8)\n",
    "                                    .transpose(1, 2, 0), affine_matrix)    \n",
    "        file_output_dir = test_results_path + os.sep + img_name + \".gz\"\n",
    "\n",
    "        # Save the NIfTI image\n",
    "        nib.save(nifti_img, file_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "save_test_results(f, ld, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "optimizer = torch.optim.SGD(ld.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda(non_blocking=True) \n",
    "\n",
    "    features = f(i)\n",
    "    output = ld(features)\n",
    "    \n",
    "    output = torch.cat(output, dim=0)\n",
    "    t = torch.cat(t, dim=0)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(output, t.cuda(non_blocking=True).type(torch.int64))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    optimizer.step()\n",
    "    # labels = t.view(-1, t.shape[-1], t.shape[-1])\n",
    "    # losses = nn.CrossEntropyLoss()(output.view(-1, 14, labels.shape[-1], labels.shape[-1]), labels)\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ld = LinearDecoder(in_channels=embed_dim, num_classes=3, is_3d=True)\n",
    "ld = ld.cuda()\n",
    "\n",
    "o = ld(features)\n",
    "print(len(o))\n",
    "print(o.shape)\n",
    "o = torch.stack([torch.nn.functional.interpolate(batch_output, size=448, mode=\"bilinear\", align_corners=False)\n",
    "                for batch_output in torch.unbind(o, dim=0)], dim=0)\n",
    "# ou = torch.nn.functional.interpolate(o[0], size=448, mode=\"bilinear\", align_corners=False)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
