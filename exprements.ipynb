{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import IPython \n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray\n",
    "from dinov2.data.transforms import make_xray_classification_eval_transform, make_classification_eval_transform\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import ModelWithNormalize, evaluate, extract_features\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230813 21:39:27 683 dinov2 config.py:60] git:\n",
      "  sha: e326bc6424a2557c160a3cdb326324ed5f8f1ebe, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20230813 21:39:27 683 dinov2 config.py:61] batch_size: 8\n",
      "comment: \n",
      "config_file: dinov2/configs/eval/vits14_pretrain.yaml\n",
      "exclude: \n",
      "gather_on_cpu: False\n",
      "n_per_class_list: [-1]\n",
      "n_tries: 1\n",
      "nb_knn: [5, 20, 50, 100, 200]\n",
      "ngpus: 1\n",
      "nodes: 1\n",
      "opts: ['train.output_dir=/mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn']\n",
      "output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "partition: learnlab\n",
      "pretrained_weights: models/dinov2_vits14_pretrain.pth\n",
      "temperature: 0.07\n",
      "timeout: 2800\n",
      "train_dataset_str: NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH/train_tmp\n",
      "use_volta32: False\n",
      "val_dataset_str: NIHChestXray:split=VAL:root=/mnt/d/data/NIH/test_tmp\n",
      "I20230813 21:39:27 683 dinov2 config.py:27] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20230813 21:39:27 683 dinov2 config.py:34] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: ImageNet:split=TRAIN\n",
      "  output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_small\n",
      "  patch_size: 14\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 518\n",
      "  local_crops_size: 98\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "I20230813 21:39:27 683 dinov2 vision_transformer.py:110] using MLP layer as FFN\n",
      "I20230813 21:39:28 683 dinov2 utils.py:35] Pretrained weights found at models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH/train_tmp', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH/test_tmp', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230813 21:39:31 683 dinov2 loaders.py:89] using dataset: \"NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH/train_tmp\"\n",
      "I20230813 21:39:34 683 dinov2 nih_chest_xray.py:67] 86480 x-ray's are missing from TRAIN set\n",
      "I20230813 21:39:34 683 dinov2 loaders.py:94] # of dataset samples: 44\n",
      "I20230813 21:39:34 683 dinov2 loaders.py:89] using dataset: \"NIHChestXray:split=VAL:root=/mnt/d/data/NIH/test_tmp\"\n",
      "I20230813 21:39:38 683 dinov2 nih_chest_xray.py:67] 86480 x-ray's are missing from VAL set\n",
      "I20230813 21:39:38 683 dinov2 loaders.py:94] # of dataset samples: 44\n"
     ]
    }
   ],
   "source": [
    "transform = make_classification_eval_transform()\n",
    "train_dataset = make_dataset(\n",
    "    dataset_str=args.train_dataset_str,\n",
    "    transform=transform,\n",
    ")\n",
    "val_dataset = make_dataset(\n",
    "    dataset_str=args.val_dataset_str,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_str = args.train_dataset_str\n",
    "val_dataset_str = args.val_dataset_str\n",
    "batch_size = args.batch_size\n",
    "gather_on_cpu = args.gather_on_cpu\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230813 21:39:38 683 dinov2 loaders.py:164] sampler: none\n",
      "I20230813 21:39:38 683 dinov2 loaders.py:211] using PyTorch data loader\n",
      "I20230813 21:39:38 683 dinov2 loaders.py:224] # of batches: 6\n",
      "I20230813 21:39:39 683 dinov2 utils.py:139] Storing features into tensor of shape torch.Size([44, 384])\n",
      "I20230813 21:39:39 683 dinov2 helpers.py:103]   [0/6]  eta: 0:00:09    time: 1.540404  data: 0.497542  max mem: 160\n",
      "I20230813 21:39:42 683 dinov2 helpers.py:103]   [5/6]  eta: 0:00:00    time: 0.792999  data: 0.611803  max mem: 162\n",
      "I20230813 21:39:42 683 dinov2 helpers.py:131]  Total time: 0:00:04 (0.793415 s / it)\n",
      "I20230813 21:39:42 683 dinov2 utils.py:151] Features shape: (44, 384)\n",
      "I20230813 21:39:42 683 dinov2 utils.py:152] Labels shape: (44, 10)\n",
      "I20230813 21:39:42 683 dinov2 loaders.py:164] sampler: none\n",
      "I20230813 21:39:42 683 dinov2 loaders.py:211] using PyTorch data loader\n",
      "I20230813 21:39:42 683 dinov2 loaders.py:224] # of batches: 6\n",
      "I20230813 21:39:43 683 dinov2 utils.py:139] Storing features into tensor of shape torch.Size([44, 384])\n",
      "I20230813 21:39:43 683 dinov2 helpers.py:103]   [0/6]  eta: 0:00:03    time: 0.509384  data: 0.499772  max mem: 162\n",
      "I20230813 21:39:46 683 dinov2 helpers.py:103]   [5/6]  eta: 0:00:00    time: 0.650439  data: 0.643776  max mem: 162\n",
      "I20230813 21:39:46 683 dinov2 helpers.py:131]  Total time: 0:00:03 (0.650837 s / it)\n",
      "I20230813 21:39:46 683 dinov2 utils.py:151] Features shape: (44, 384)\n",
      "I20230813 21:39:46 683 dinov2 utils.py:152] Labels shape: (44, 10)\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast(dtype=autocast_dtype):\n",
    "    train_features, train_labels = extract_features(\n",
    "        model, train_dataset, batch_size, num_workers, gather_on_cpu=gather_on_cpu\n",
    "    )\n",
    "    model.eval()\n",
    "    val_features, val_labels = extract_features(\n",
    "        model, val_dataset, batch_size, num_workers, gather_on_cpu=gather_on_cpu\n",
    "    )\n",
    "\n",
    "train_features = train_features.cpu().numpy()\n",
    "train_labels = train_labels.cpu().numpy()\n",
    "val_features = val_features.cpu().numpy()\n",
    "val_labels = val_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Optional\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.wrappers import ClasswiseWrapper\n",
    "from torchmetrics.classification import (MultilabelAUROC, MultilabelF1Score, MultilabelAccuracy, MulticlassF1Score,\n",
    "                                        MulticlassAccuracy, MulticlassAUROC, Accuracy, BinaryF1Score, BinaryAUROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MACRO = \"macro\"\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_multilabel_auroc_metric(average_type: MetricAveraging, num_labels: int, labels=None):\n",
    "    metrics: Dict[str, Metric] = {\n",
    "        f\"auroc\": MultilabelAUROC(num_labels=num_labels, average=average_type.value),\n",
    "        \"class-specific\": MetricCollection({\n",
    "            \"auroc\": ClasswiseWrapper(MultilabelAUROC(num_labels=num_labels, average=None), labels=labels, prefix=\"_\"),\n",
    "        }) \n",
    "    }\n",
    "    return MetricCollection(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = build_multilabel_auroc_metric(MetricAveraging.MACRO, 10, [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in metric.values():\n",
    "    m = m.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = np.expand_dims(val_labels[0, :], axis=0)\n",
    "preds = np.expand_dims(np.array([0.7, 0.5, 0.4, 0.4, 0.4, 0.3, 0.8, 0.2, 0.1, 0.5]), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\"target\": torch.tensor(tar, device='cuda'), \"preds\": torch.tensor(preds, device='cuda')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], device='cuda:0'),\n",
       " 'preds': tensor([[0.7000, 0.5000, 0.4000, 0.4000, 0.4000, 0.3000, 0.8000, 0.2000, 0.1000,\n",
       "          0.5000]], device='cuda:0', dtype=torch.float64)}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], device='cuda:0'), 'preds': tensor([[0.7000, 0.5000, 0.4000, 0.4000, 0.4000, 0.3000, 0.8000, 0.2000, 0.1000,\n",
      "         0.5000]], device='cuda:0', dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "print({**res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.update(**res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('auroc', MultilabelAUROC())\n",
      "('class-specific_auroc', ClasswiseWrapper(\n",
      "  (metric): MultilabelAUROC()\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "for i in metric.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230813 21:39:38 683 dinov2 loaders.py:164] sampler: none\n",
      "I20230813 21:39:38 683 dinov2 loaders.py:211] using PyTorch data loader\n",
      "I20230813 21:39:38 683 dinov2 loaders.py:224] # of batches: 6\n",
      "I20230813 21:39:39 683 dinov2 utils.py:139] Storing features into tensor of shape torch.Size([44, 384])\n",
      "I20230813 21:39:39 683 dinov2 helpers.py:103]   [0/6]  eta: 0:00:09    time: 1.540404  data: 0.497542  max mem: 160\n",
      "I20230813 21:39:42 683 dinov2 helpers.py:103]   [5/6]  eta: 0:00:00    time: 0.792999  data: 0.611803  max mem: 162\n",
      "I20230813 21:39:42 683 dinov2 helpers.py:131]  Total time: 0:00:04 (0.793415 s / it)\n",
      "I20230813 21:39:42 683 dinov2 utils.py:151] Features shape: (44, 384)\n",
      "I20230813 21:39:42 683 dinov2 utils.py:152] Labels shape: (44, 10)\n",
      "I20230813 21:39:42 683 dinov2 loaders.py:164] sampler: none\n",
      "I20230813 21:39:42 683 dinov2 loaders.py:211] using PyTorch data loader\n",
      "I20230813 21:39:42 683 dinov2 loaders.py:224] # of batches: 6\n",
      "I20230813 21:39:43 683 dinov2 utils.py:139] Storing features into tensor of shape torch.Size([44, 384])\n",
      "I20230813 21:39:43 683 dinov2 helpers.py:103]   [0/6]  eta: 0:00:03    time: 0.509384  data: 0.499772  max mem: 162\n",
      "I20230813 21:39:46 683 dinov2 helpers.py:103]   [5/6]  eta: 0:00:00    time: 0.650439  data: 0.643776  max mem: 162\n",
      "I20230813 21:39:46 683 dinov2 helpers.py:131]  Total time: 0:00:03 (0.650837 s / it)\n",
      "I20230813 21:39:46 683 dinov2 utils.py:151] Features shape: (44, 384)\n",
      "I20230813 21:39:46 683 dinov2 utils.py:152] Labels shape: (44, 10)\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast(dtype=autocast_dtype):\n",
    "    train_features, train_labels = extract_features(\n",
    "        model, train_dataset, batch_size, num_workers, gather_on_cpu=gather_on_cpu\n",
    "    )\n",
    "    model.eval()\n",
    "    val_features, val_labels = extract_features(\n",
    "        model, val_dataset, batch_size, num_workers, gather_on_cpu=gather_on_cpu\n",
    "    )\n",
    "\n",
    "train_features = train_features.cpu().numpy()\n",
    "train_labels = train_labels.cpu().numpy()\n",
    "val_features = val_features.cpu().numpy()\n",
    "val_labels = val_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\"a\": metric}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('auroc', MultilabelAUROC()), ('class-specific_auroc', ClasswiseWrapper(\n",
       "  (metric): MultilabelAUROC()\n",
       "))])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       " 'class-specific_auroc': {'_a': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_b': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_c': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_d': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_e': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_f': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_g': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_h': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_i': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  '_j': tensor(0., device='cuda:0', dtype=torch.float64)}}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = {k: m.compute() for k, m in metric.items()}\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_method_to_nested_values(d, method_name):\n",
    "    result = {}\n",
    "    print(d)\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, MetricCollection):\n",
    "            result[key] = apply_method_to_nested_values(value, method_name)\n",
    "        else:\n",
    "            method = getattr(value, method_name)\n",
    "            result[key] = method()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {'auroc': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "  'class-specific_auroc': {'_a': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_b': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_c': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_d': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_e': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_f': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_g': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_h': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_i': tensor(0., device='cuda:0', dtype=torch.float64),\n",
       "   '_j': tensor(0., device='cuda:0', dtype=torch.float64)}}}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Atelectasis', 'Cardiomegaly', 'Effusion', 'Emphysema', 'Fibrosis',\n",
       "       'Infiltration', 'Mass', 'No Finding', 'Nodule',\n",
       "       'Pleural_Thickening'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = build_multilabel_auroc_metric(MetricAveraging.MACRO, 10, list(train_dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from dinov2.eval.utils import MLkNN\n",
    "\n",
    "\n",
    "classifier = MLkNN(5)\n",
    "classifier.fit(train_features, train_labels)\n",
    "results = classifier.predict_proba(val_features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from dinov2.eval.utils import MLkNN\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for k in [20]:\n",
    "    results_dict[f\"{k}\"] = {}\n",
    "\n",
    "    classifier = MLkNN(k)\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    results = torch.tensor(classifier.predict_proba(val_features).toarray(), dtype=torch.float64).cuda()\n",
    "\n",
    "    metric.update(**{\"target\": torch.tensor(val_labels).cuda(), \"preds\": results})\n",
    "\n",
    "    # Disease-specific scores\n",
    "    # disease_results = {\"AUC\": {}, \"Accuracy\": {}, \"F1\": {}}\n",
    "    # for index, disease in enumerate(train_dataset.class_names):\n",
    "    #     disease_results[\"AUC\"][disease] =  sklearn.metrics.roc_auc_score(val_labels[:, index], results[:, index])\n",
    "\n",
    "    # results_dict[f\"{k}\"][\"Disease-specific\"] = disease_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': tensor(0.5880, device='cuda:0'),\n",
       " '_Atelectasis': tensor(0.6365, device='cuda:0'),\n",
       " '_Cardiomegaly': tensor(0.9884, device='cuda:0'),\n",
       " '_Effusion': tensor(0.2558, device='cuda:0'),\n",
       " '_Emphysema': tensor(0.5943, device='cuda:0'),\n",
       " '_Fibrosis': tensor(0.6820, device='cuda:0'),\n",
       " '_Infiltration': tensor(0.4884, device='cuda:0'),\n",
       " '_Mass': tensor(0.5250, device='cuda:0'),\n",
       " '_No Finding': tensor(0.8837, device='cuda:0'),\n",
       " '_Nodule': tensor(0.5993, device='cuda:0'),\n",
       " '_Pleural_Thickening': tensor(0.2262, device='cuda:0')}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from dinov2.eval.utils import MLkNN\n",
    "\n",
    "ab = {}\n",
    "\n",
    "for k in [20]:\n",
    "    ab[f\"{k}\"] = {}\n",
    "\n",
    "    classifier = MLkNN(k)\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    results = classifier.predict_proba(val_features).toarray()\n",
    "\n",
    "    ab[f\"{k}\"][\"mAUC Combined\"]  = sklearn.metrics.roc_auc_score(val_labels, results, average=\"macro\")\n",
    "\n",
    "    disease_results = {\"AUC\": {}, \"Accuracy\": {}, \"F1\": {}}\n",
    "    for index, disease in enumerate(train_dataset.class_names):\n",
    "        disease_results[\"AUC\"][disease] =  sklearn.metrics.roc_auc_score(val_labels[:, index], results[:, index])\n",
    "\n",
    "    ab[f\"{k}\"][\"Disease-specific\"] = disease_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m results_dict[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39maccuracy_score(val_labels, results\u001b[39m>\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m     17\u001b[0m results_dict[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmAUC Combined\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mroc_auc_score(val_labels, results, average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m results_dict[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mF1\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39;49mmetrics\u001b[39m.\u001b[39;49mf1_score(val_labels, results, average\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmacro\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Disease-specific scores\u001b[39;00m\n\u001b[1;32m     21\u001b[0m disease_results \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mAUC\u001b[39m\u001b[39m\"\u001b[39m: {}, \u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m: {}, \u001b[39m\"\u001b[39m\u001b[39mF1\u001b[39m\u001b[39m\"\u001b[39m: {}}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1238\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m   1071\u001b[0m     {\n\u001b[1;32m   1072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1097\u001b[0m ):\n\u001b[1;32m   1098\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \n\u001b[1;32m   1100\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[1;32m   1239\u001b[0m         y_true,\n\u001b[1;32m   1240\u001b[0m         y_pred,\n\u001b[1;32m   1241\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1242\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1243\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1244\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1245\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1246\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1247\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1411\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m   1251\u001b[0m     {\n\u001b[1;32m   1252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1279\u001b[0m ):\n\u001b[1;32m   1280\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \n\u001b[1;32m   1282\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[39m    0.38...\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1411\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1412\u001b[0m         y_true,\n\u001b[1;32m   1413\u001b[0m         y_pred,\n\u001b[1;32m   1414\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m   1415\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1416\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1417\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1418\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1419\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1420\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1421\u001b[0m     )\n\u001b[1;32m   1422\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1721\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m \n\u001b[1;32m   1565\u001b[0m \u001b[39mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[39m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m zero_division_value \u001b[39m=\u001b[39m _check_zero_division(zero_division)\n\u001b[0;32m-> 1721\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1723\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1499\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1499\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1500\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from dinov2.eval.utils import MLkNN\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for k in args.nb_knn:\n",
    "    results_dict[f\"{k}\"] = {}\n",
    "\n",
    "    classifier = MLkNN(k)\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    results = classifier.predict_proba(val_features).toarray()\n",
    "    \n",
    "    results_dict[f\"{k}\"][\"Hamming Loss\"]  = sklearn.metrics.hamming_loss(val_labels, results>0.5)\n",
    "    results_dict[f\"{k}\"][\"Accuracy\"]  = sklearn.metrics.accuracy_score(val_labels, results>0.5)\n",
    "    results_dict[f\"{k}\"][\"mAUC Combined\"]  = sklearn.metrics.roc_auc_score(val_labels, results, average=\"macro\")\n",
    "    results_dict[f\"{k}\"][\"F1\"]  = sklearn.metrics.f1_score(val_labels, results, average=\"macro\")\n",
    "\n",
    "    # Disease-specific scores\n",
    "    disease_results = {\"AUC\": {}, \"Accuracy\": {}, \"F1\": {}}\n",
    "    for index, disease in enumerate(train_dataset.class_names):\n",
    "        disease_results[\"AUC\"][disease] =  sklearn.metrics.roc_auc_score(val_labels[:, index], results[:, index])\n",
    "        disease_results[\"Accuracy\"][disease] =  sklearn.metrics.accuracy_score(val_labels[:, index], results[:, index])\n",
    "        disease_results[\"F1\"][disease] =  sklearn.metrics.f1_score(val_labels[:, index], results[:, index])\n",
    "\n",
    "    results_dict[f\"{k}\"][\"Disease-specific\"] = disease_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
