{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_zero_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20231001 18:01:44 27101 dinov2 config.py:60] git:\n",
      "  sha: 047962eee435bc0d1094fe0f4324d5033bca773a, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20231001 18:01:44 27101 dinov2 config.py:61] batch_size: 8\n",
      "comment: \n",
      "config_file: dinov2/configs/eval/vits14_pretrain.yaml\n",
      "exclude: \n",
      "gather_on_cpu: False\n",
      "n_per_class_list: [-1]\n",
      "n_tries: 1\n",
      "nb_knn: [5, 20, 50, 100, 200]\n",
      "ngpus: 1\n",
      "nodes: 1\n",
      "opts: ['train.output_dir=/mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn']\n",
      "output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "partition: learnlab\n",
      "pretrained_weights: models/dinov2_vits14_pretrain.pth\n",
      "temperature: 0.07\n",
      "test_dataset_str: MC:split=TEST:root=/mnt/z/data/MC\n",
      "timeout: 2800\n",
      "train_dataset_str: MC:split=TRAIN:root=/mnt/z/data/MC\n",
      "use_volta32: False\n",
      "val_dataset_str: MC:split=VAL:root=/mnt/z/data/MC\n",
      "I20231001 18:01:44 27101 dinov2 config.py:27] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20231001 18:01:44 27101 dinov2 config.py:34] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: ImageNet:split=TRAIN\n",
      "  output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_small\n",
      "  patch_size: 14\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 518\n",
      "  local_crops_size: 98\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "I20231001 18:01:44 27101 dinov2 vision_transformer.py:110] using MLP layer as FFN\n",
      "I20231001 18:01:45 27101 dinov2 utils.py:35] Pretrained weights found at models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/mnt/d/data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='MC:split=TRAIN:root=/mnt/z/data/MC', val_dataset_str='MC:split=VAL:root=/mnt/z/data/MC', test_dataset_str='MC:split=TEST:root=/mnt/z/data/MC', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "feature_model_with_inter = ModelWithIntermediateLayers(model, 4, autocast_ctx, is_3d=False)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_str = args.train_dataset_str\n",
    "val_dataset_str = args.val_dataset_str\n",
    "batch_size = args.batch_size\n",
    "gather_on_cpu = args.gather_on_cpu\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20231001 18:06:41 27101 dinov2 loaders.py:96] using dataset: \"MC:split=TRAIN:root=/mnt/z/data/MC\"\n",
      "I20231001 18:06:41 27101 dinov2 medical_dataset.py:36] 0 scans are missing from TRAIN set\n",
      "I20231001 18:06:41 27101 dinov2 loaders.py:101] # of dataset samples: 69\n",
      "I20231001 18:06:41 27101 dinov2 loaders.py:96] using dataset: \"MC:split=VAL:root=/mnt/z/data/MC\"\n",
      "I20231001 18:06:41 27101 dinov2 medical_dataset.py:36] 0 scans are missing from VAL set\n",
      "I20231001 18:06:41 27101 dinov2 loaders.py:101] # of dataset samples: 23\n",
      "I20231001 18:06:41 27101 dinov2 utils.py:338] Train and val datasets have been combined.\n",
      "I20231001 18:06:41 27101 dinov2 loaders.py:96] using dataset: \"MC:split=TEST:root=/mnt/z/data/MC\"\n",
      "I20231001 18:06:41 27101 dinov2 medical_dataset.py:36] 0 scans are missing from TEST set\n",
      "I20231001 18:06:41 27101 dinov2 loaders.py:101] # of dataset samples: 46\n"
     ]
    }
   ],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "# train_image_transform = make_classification_train_transform()\n",
    "# eval_image_transform = make_classification_eval_transform()\n",
    "train_target_transform = eval_target_transform = None\n",
    "\n",
    "# val_dataset_str = args.val_dataset_str\n",
    "val_dataset_str = None\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=args.train_dataset_str, val_dataset_str=val_dataset_str,\n",
    "                                                        test_dataset_str=args.test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "\n",
    "# sampler_type = SamplerType.INFINITE\n",
    "sampler_type = None\n",
    "\n",
    "is_3d = test_dataset.is_3d()\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=train_dataset,\n",
    "#     batch_size=2,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "# val_data_loader = make_data_loader(\n",
    "#     dataset=val_dataset,\n",
    "#     batch_size=1,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoderUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, embed_dim=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*2, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.skip_conv = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upconv(x1)\n",
    "        x2 = self.skip_conv(x2)\n",
    "        scale_factor = (x1.size()[2] / x2.size()[2])\n",
    "        x2 = nn.Upsample(scale_factor=scale_factor, mode=\"bilinear\", align_corners=True)(x2)\n",
    "        x = torch.concat([x1, x2], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, image_size=224):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.embed_dim = in_channels\n",
    "        self.image_size = image_size\n",
    "        self.out_channels = out_channels\n",
    "        self.up1 = UNetDecoderUpBlock(in_channels=in_channels, out_channels=in_channels//2, embed_dim=embed_dim)\n",
    "        self.up2 = UNetDecoderUpBlock(in_channels=in_channels//2, out_channels=in_channels//4, embed_dim=embed_dim)\n",
    "        self.up3 = UNetDecoderUpBlock(in_channels=in_channels//4, out_channels=in_channels//8, embed_dim=embed_dim)\n",
    "        self.up4 = UNetDecoderUpBlock(in_channels=in_channels//8, out_channels=out_channels, embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = w = self.image_size//14\n",
    "\n",
    "        skip1 = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip2 = x[2].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip3 = x[1].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip4 = x[0].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        x1    = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        \n",
    "        x2 = self.up1(x1, skip1)\n",
    "        x3 = self.up2(x2, skip2)\n",
    "        x4 = self.up3(x3, skip3)\n",
    "        x5 = self.up4(x4, skip4)\n",
    "\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=4, is_3d=False).cuda()\n",
    "decoder = UNetDecoder(in_channels=model.embed_dim, out_channels=3).cuda()\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=False)\n",
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    embeddings = feature_model(i)\n",
    "    output = decoder(embeddings)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3750, -3.3723, -1.9497,  ..., -1.3268, -0.9333, -0.9981],\n",
      "         [ 0.4478, -2.9133, -2.2538,  ..., -0.8136, -0.2220,  0.4753],\n",
      "         [ 0.5773, -3.3643, -2.7862,  ..., -1.4565, -0.9303,  2.9201],\n",
      "         ...,\n",
      "         [-0.5332, -2.1409, -1.9454,  ..., -2.2346, -2.5651, -4.9458],\n",
      "         [-0.7281, -0.2295, -3.1759,  ..., -0.9039, -1.8940, -5.0801],\n",
      "         [-0.3124,  0.9867, -2.3786,  ...,  0.0680, -1.5946, -4.7298]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((tensor([[[ 0.3754, -3.3728, -1.9493,  ..., -1.3275, -0.9343, -0.9976],\n",
      "         [ 0.4481, -2.9115, -2.2523,  ..., -0.8120, -0.2235,  0.4772],\n",
      "         [ 0.5807, -3.3610, -2.7855,  ..., -1.4568, -0.9304,  2.9267],\n",
      "         ...,\n",
      "         [-0.5295, -2.1414, -1.9441,  ..., -2.2333, -2.5661, -4.9458],\n",
      "         [-0.7259, -0.2327, -3.1773,  ..., -0.9081, -1.8951, -5.0850],\n",
      "         [-0.3152,  0.9831, -2.3822,  ...,  0.0659, -1.5966, -4.7304]]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>), tensor([[ 5.7421e-01,  2.7907e+00, -1.8493e+00,  4.8371e+00, -3.4320e+00,\n",
      "         -1.6165e+00, -2.8376e-01,  2.1611e+00,  3.0133e+00, -9.4044e-02,\n",
      "          5.0435e-01,  2.3822e+00,  1.7914e-01,  3.8141e+00,  3.5105e+00,\n",
      "          4.8408e-01, -3.6066e+00, -3.7887e+00, -7.2021e-02, -1.6886e+00,\n",
      "          1.5435e+00, -2.4341e+00,  3.3907e+00, -5.4760e-01,  9.1488e-01,\n",
      "          2.1417e+00,  1.3259e+00,  2.3891e+00, -1.5123e-01,  6.6424e-01,\n",
      "          5.6075e+00, -6.7871e-01,  1.7857e+00,  2.4257e+00,  1.8980e+00,\n",
      "         -3.8371e+00, -2.7448e+00, -3.6490e+00, -3.9366e+00,  1.3237e+00,\n",
      "         -2.7819e+00,  1.8404e+00, -3.3669e+00,  5.1453e+00,  1.0227e-01,\n",
      "         -2.1361e+00, -1.8967e+00, -1.7103e+00,  9.8421e-01, -2.4247e+00,\n",
      "         -2.5506e+00, -3.6192e-01,  5.0119e+00,  2.2509e+00, -1.5749e+00,\n",
      "         -3.5789e-01,  2.0690e+00,  1.4209e+00, -1.3783e+00,  2.3757e-01,\n",
      "          9.2467e-01, -7.1627e-01,  2.4182e+00, -1.7761e+00, -1.5473e+00,\n",
      "         -4.0751e+00,  4.2002e+00,  3.0209e-01, -1.1129e+00,  1.3865e+00,\n",
      "          1.4777e+00,  1.9766e+00,  3.1307e-01, -3.5129e+00, -1.2638e+00,\n",
      "         -6.1797e-01, -3.7008e+00,  1.4215e+00,  3.6730e+00, -3.6464e-01,\n",
      "         -1.0614e+00,  1.5085e-01,  3.5441e-01, -5.1655e+00,  7.9455e-01,\n",
      "          1.9184e+00, -7.1698e-01,  3.4016e+00, -3.2411e+00,  3.0290e+00,\n",
      "         -3.2840e-01,  3.8617e+00, -6.7326e-02, -1.7257e+00,  4.9673e+00,\n",
      "         -2.5172e+00, -3.3908e+00, -1.5124e+00, -4.8031e+00,  2.3342e+00,\n",
      "          2.1868e+00, -4.6439e-01, -1.6736e+00,  6.8008e-01,  6.2303e+00,\n",
      "         -6.5423e-01, -1.3584e+00,  4.7768e+00,  1.6238e+00, -7.8366e-01,\n",
      "         -3.9813e+00,  8.5271e-01, -5.0646e-02,  1.2070e+00,  9.6369e-01,\n",
      "         -1.6890e-01,  2.5662e+00,  1.1126e+00, -6.5075e+00, -1.0599e+00,\n",
      "          2.9533e+00, -1.7169e-01,  3.1210e+00, -3.4701e+00,  1.8535e+00,\n",
      "         -2.9015e-01, -1.3540e+00, -1.3007e+00, -7.5739e+00, -2.3838e+00,\n",
      "         -1.8860e+00,  1.7022e+00,  1.4409e+00, -1.2504e+00,  2.0168e+00,\n",
      "          2.0475e+00, -4.5075e+00, -3.0693e+00,  2.9253e+00,  2.6603e+00,\n",
      "          1.7821e+00, -2.4385e-01,  5.6173e-01, -1.0842e+00, -1.3292e+00,\n",
      "          2.2179e-02, -9.2223e-01,  3.2247e+00,  2.8723e+00,  1.3732e+00,\n",
      "         -2.0968e+00,  1.8994e+00,  1.8237e+00,  2.4098e+00,  2.2551e+00,\n",
      "         -5.2710e+00,  1.0361e+00,  1.5728e-01, -9.7167e-01, -1.6213e+00,\n",
      "         -4.0512e+00,  3.5000e-01,  5.3203e-01,  1.6781e+00, -2.0490e+00,\n",
      "          5.2780e-01,  2.3949e+00, -3.9555e-01,  1.0180e+00, -7.5551e-03,\n",
      "          1.7235e+00, -6.8715e-01,  2.8479e+00, -2.7749e+00, -4.2922e+00,\n",
      "          3.9504e+00,  7.2557e-01,  2.3471e+00,  1.9730e+00,  3.3021e+00,\n",
      "         -7.7145e-02,  1.3885e+00,  2.2031e+00,  1.8039e+00, -3.6235e+00,\n",
      "          1.5583e+00, -6.0101e-01,  1.7150e-01,  2.3534e+00, -6.5502e-01,\n",
      "         -1.0237e+00, -3.1822e+00, -1.7957e+00,  7.7065e-01, -2.8990e+00,\n",
      "          3.1243e+00,  1.7405e-01,  2.5196e+00, -3.3414e+00, -1.3815e+00,\n",
      "         -3.3503e+00, -1.9706e+00, -1.2532e+00,  2.2544e+00, -4.0055e+00,\n",
      "          4.9312e+00,  3.2780e+00, -5.8621e-01,  5.7322e+00, -8.0835e-01,\n",
      "         -3.6177e+00, -1.6044e+00, -2.6068e-01,  1.7629e+00, -1.2650e+00,\n",
      "          1.9231e+00, -8.8615e-01,  1.1858e+00, -5.4090e-01,  6.1216e-01,\n",
      "          2.5428e+00,  5.2638e+00, -3.1440e+00, -1.1260e+00, -2.7985e+00,\n",
      "         -1.3285e+00, -1.2477e+00,  8.3245e-01,  3.5493e+00, -2.3205e+00,\n",
      "         -3.1899e+00,  3.8848e+00, -1.0734e+00,  1.2132e+00, -6.7469e-01,\n",
      "          2.3531e+00,  3.2197e-01, -4.0505e-01, -3.5925e-02,  1.6444e+00,\n",
      "          8.0432e-01,  1.0373e+00, -3.8469e+00,  7.8583e-01, -5.5634e-02,\n",
      "          1.6964e-01,  6.1881e-01, -4.7942e+00, -2.7409e+00,  4.1871e+00,\n",
      "          1.5874e+00,  3.7953e+00,  6.5836e-02, -2.2296e+00, -1.4489e+00,\n",
      "          3.3219e-01,  2.8642e+00,  1.5563e+00,  4.6812e+00, -3.2454e+00,\n",
      "         -1.2003e+00,  3.0247e+00,  3.8901e+00,  1.0493e+00,  3.5486e-01,\n",
      "          2.4884e+00, -2.7443e+00, -6.1514e+00, -1.8368e+00,  3.4002e+00,\n",
      "         -2.5514e+00, -1.9120e+00, -1.4569e+00,  2.2779e+00, -1.8369e+00,\n",
      "         -1.9693e+00, -3.0217e+00, -1.6509e+00,  6.6259e-01,  2.0462e+00,\n",
      "         -8.7187e-01,  1.2687e+00,  4.1148e+00, -1.7322e-01, -3.3435e+00,\n",
      "          4.2103e+00, -3.1382e+00,  2.8568e+00,  1.0188e+00, -1.9091e+00,\n",
      "         -3.5676e+00, -1.0435e+00,  4.2904e-01, -3.0998e+00, -2.8529e+00,\n",
      "          1.3344e+00, -5.5021e-01,  1.9895e+00,  4.0328e-01, -3.5971e+00,\n",
      "         -3.5555e-01,  1.6929e+00, -2.8313e+00, -1.4654e-01, -1.1379e+00,\n",
      "          2.2539e+00,  5.4071e+00,  2.1086e+00,  2.3258e+00,  1.4787e+00,\n",
      "          3.8374e-01,  1.9910e-01, -1.0498e-01,  2.5893e+00,  3.7386e+00,\n",
      "          1.9694e+00, -8.7635e-01,  2.4881e-01, -3.9025e+00,  9.5645e-01,\n",
      "          9.7056e-01, -2.1501e+00, -2.7224e+00,  3.0493e+00, -2.9845e+00,\n",
      "         -3.4347e+00, -7.7322e-01,  1.3597e+00, -1.8683e+00,  3.1659e+00,\n",
      "         -6.8992e+00,  1.1454e+00, -1.3215e+00, -6.5030e-01,  1.0037e+00,\n",
      "         -4.3923e+00, -1.0663e+00,  3.5343e+00, -1.5233e+00,  2.1075e+00,\n",
      "          2.7330e+00, -4.5557e+00, -3.4612e+00,  2.3548e+00, -7.2475e-01,\n",
      "          1.8555e+00,  1.0240e+00, -6.5168e-02,  1.4725e+00, -2.8280e-01,\n",
      "         -1.7782e+00, -1.3570e+00, -5.6428e+00, -1.9140e-01, -4.4210e+00,\n",
      "          8.9817e-01,  2.5822e+00,  1.4489e-02,  4.1859e+00,  2.4220e+00,\n",
      "         -1.7805e+00, -2.4674e+00, -1.4611e+00,  1.6861e+00, -2.6244e+00,\n",
      "         -9.9870e-02, -3.5383e+00,  3.2865e+00, -2.2640e+00, -1.7730e+00,\n",
      "          1.0902e+00,  3.0616e+00,  7.0529e-01, -5.4338e-01,  7.3753e-01,\n",
      "         -3.5710e+00, -8.7125e-01, -2.9747e+00,  2.6617e+00, -1.3266e+00,\n",
      "          9.8795e-01, -7.3318e-01, -3.0816e+00,  1.1326e+00]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)),)]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_dataset.get_image_data(0)\n",
    "lbl = test_dataset.get_target(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOV2Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, autocast_ctx, is_3d=False) -> None:\n",
    "        super(DINOV2Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "        self.is_3d = is_3d\n",
    "    \n",
    "    def forward_3d(self, x):\n",
    "        batch_features = [] \n",
    "        for batch_scans in x: # calculate the features for every scan in all scans of the batch\n",
    "            scans = []\n",
    "            for scan in batch_scans:\n",
    "                if not is_zero_matrix(scan): scans.append(self.forward_(scan.unsqueeze(0)))\n",
    "            batch_features.append(scans)\n",
    "        return batch_features\n",
    "\n",
    "    def forward_(self, x):\n",
    "        with torch.no_grad():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.encoder.forward_features(x)['x_norm_patchtokens']\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if is_3d:\n",
    "            return self.forward_3d(x)\n",
    "        return self.forward_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in test_dataset:\n",
    "    show_image_from_tensor(i[0] * 100)\n",
    "    show_image_from_tensor(i[1] * 100)\n",
    "    show_image_from_tensor(i[2] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_results(feature_model, decoder, dataset):\n",
    "    for i, (img, _) in enumerate(dataset):\n",
    "\n",
    "        img_name = test_dataset.images[i]\n",
    "        _, affine_matrix = test_dataset.get_image_data(i, return_affine_matrix=True)\n",
    "\n",
    "        img = img.cuda(non_blocking=True) \n",
    "\n",
    "        features = feature_model(img.unsqueeze(0))\n",
    "        output = decoder(features, up_size=512)[0]\n",
    "        output = output.argmax(dim=1)\n",
    "\n",
    "        nifti_img = nib.Nifti1Image(output\n",
    "                                    .cpu()\n",
    "                                    .numpy()\n",
    "                                    .astype(np.uint8)\n",
    "                                    .transpose(1, 2, 0), affine_matrix)    \n",
    "        file_output_dir = test_results_path + os.sep + img_name + \".gz\"\n",
    "\n",
    "        # Save the NIfTI image\n",
    "        nib.save(nifti_img, file_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "save_test_results(f, ld, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "optimizer = torch.optim.SGD(ld.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda(non_blocking=True) \n",
    "\n",
    "    features = f(i)\n",
    "    output = ld(features)\n",
    "    \n",
    "    output = torch.cat(output, dim=0)\n",
    "    t = torch.cat(t, dim=0)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(output, t.cuda(non_blocking=True).type(torch.int64))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    optimizer.step()\n",
    "    # labels = t.view(-1, t.shape[-1], t.shape[-1])\n",
    "    # losses = nn.CrossEntropyLoss()(output.view(-1, 14, labels.shape[-1], labels.shape[-1]), labels)\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ld = LinearDecoder(in_channels=embed_dim, num_classes=3, is_3d=True)\n",
    "ld = ld.cuda()\n",
    "\n",
    "o = ld(features)\n",
    "print(len(o))\n",
    "print(o.shape)\n",
    "o = torch.stack([torch.nn.functional.interpolate(batch_output, size=448, mode=\"bilinear\", align_corners=False)\n",
    "                for batch_output in torch.unbind(o, dim=0)], dim=0)\n",
    "# ou = torch.nn.functional.interpolate(o[0], size=448, mode=\"bilinear\", align_corners=False)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
