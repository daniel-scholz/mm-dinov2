{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_transform, make_classification_eval_transform, make_segmentation_target_transform,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import is_zero_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d\n",
    "from dinov2.eval.classification.linear import LinearClassifier, create_linear_input\n",
    "from dinov2.eval.metrics import build_segmentation_metrics\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/mnt/d/data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "feature_model = ModelWithIntermediateLayers(model, 1, autocast_ctx)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_str = args.train_dataset_str\n",
    "val_dataset_str = args.val_dataset_str\n",
    "batch_size = args.batch_size\n",
    "gather_on_cpu = args.gather_on_cpu\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = target_transform = make_classification_train_transform()\n",
    "dataset = SARSCoV2CT(split=SARSCoV2CT.Split.TRAIN,\n",
    "                root=\"/mnt/z/data/SARS-CoV-2-CT\",\n",
    "                transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230913 18:09:14 9003 dinov2 loaders.py:169] sampler: none\n",
      "I20230913 18:09:14 9003 dinov2 loaders.py:216] using PyTorch data loader\n",
      "I20230913 18:09:14 9003 dinov2 loaders.py:229] # of batches: 23\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = make_data_loader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=collate_fn_3d,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=None,\n",
    "    sampler_advance=1,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "\n",
    "    def __init__(self, out_dim, use_n_blocks, use_avgpool, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.use_n_blocks = use_n_blocks\n",
    "        self.use_avgpool = use_avgpool\n",
    "        self.num_classes = num_classes\n",
    "        self.linear = nn.Linear(out_dim, num_classes)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x_tokens_list):\n",
    "        output = torch.stack(\n",
    "            [create_linear_input(o, self.use_n_blocks, self.use_avgpool) for o in x_tokens_list]\n",
    "            ).mean(dim=0)\n",
    "        return self.linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LinearClassifier(384, 1, False, 1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-4.7901, -1.1878, -2.7847,  ...,  1.0450, -0.8659,  0.2102],\n",
      "         [ 1.1833, -1.3809, -0.6440,  ..., -0.4353,  0.4215,  0.2689],\n",
      "         [-2.8502, -1.3084, -0.2974,  ...,  0.8173,  0.7834,  0.1212],\n",
      "         ...,\n",
      "         [ 0.7216, -1.2664, -1.6476,  ...,  0.6348,  0.0157,  0.2892],\n",
      "         [ 0.7948, -1.2640, -0.6669,  ..., -0.2051,  0.7411,  0.0309],\n",
      "         [ 0.9847, -1.1224, -1.1840,  ..., -0.7702, -0.5966,  0.5401]]],\n",
      "       device='cuda:0'), tensor([[ 5.5086,  4.9412, -0.9300, -0.6931, -0.0585, -0.0348,  0.5048,  1.7975,\n",
      "         -1.3928, -4.9028,  5.1640, -0.3574, -1.1447, -1.1179,  1.5831,  4.6362,\n",
      "         -4.6162, -2.6647,  1.4211,  1.4005, -5.6725,  2.4767, -2.4337,  3.5097,\n",
      "         -0.7909,  1.5959, -1.2211, -1.8150,  3.5725,  1.3231,  3.2760, -1.6686,\n",
      "         -0.4039,  3.5784, -3.7027,  0.3881, -2.4083,  4.5341, -2.2845,  2.3469,\n",
      "         -1.5312,  2.0159,  1.2864,  2.6986,  0.8494,  3.1104, -2.4067,  0.8887,\n",
      "          0.6338, -1.8178, -0.5770,  4.2976,  0.0197, -0.7264, -4.8797, -0.3624,\n",
      "         -0.8404,  3.2003,  0.8339,  3.7524,  7.0515,  3.7359, -0.8229, -0.7423,\n",
      "          3.0813,  1.6465, -0.4308,  3.1257, -6.5841, -4.6900, -1.0234, -2.8177,\n",
      "         -0.0541,  1.7567, -0.9618, -3.1175, -2.3324,  0.4316, -1.3864, -0.2115,\n",
      "         -1.4385, -4.0747, -0.5695,  0.2414,  4.1340, -2.5687,  3.0532,  4.0112,\n",
      "         -2.7409, -1.5446, -4.7380, -1.2687, -0.2901,  2.9280, -2.7330,  3.4864,\n",
      "          1.9128,  1.2912,  1.7847, -3.7619, -0.9175, -0.3660, -0.2891,  0.1138,\n",
      "         -1.5611, -3.3156,  1.3591, -2.8566,  2.4790, -0.2805,  0.2547, -3.0771,\n",
      "          7.1720, -2.2486, -1.8535,  8.0973, -0.8367,  4.1362, -0.9004, -0.2550,\n",
      "         -2.1265,  3.6461,  0.2922,  0.9956, -3.3702, -1.1137, -2.1626, -3.9761,\n",
      "         -3.1197,  2.1217, -3.2888, -2.4430,  1.4858, -4.1562, -1.4631,  2.8081,\n",
      "         -1.2930,  1.5346, -0.8980, -2.5981, -1.8519, -0.1826, -4.3832, -0.1565,\n",
      "          0.4985,  3.1779, -3.4504, -0.9850, -2.4543, -1.4215, -1.2578, -0.7353,\n",
      "          1.7291,  1.6058,  1.2481,  0.5394, -3.6275,  1.2400, -4.1640,  2.8494,\n",
      "         -1.9786, -0.9563,  0.2284, -1.3500,  0.3030, -1.0823,  0.9509, -0.5226,\n",
      "         -0.8445,  0.3870, -1.9252,  2.9754, -2.3016, -2.2859, -4.1851,  0.1643,\n",
      "          3.8689, -0.3096, -3.8541,  0.6112,  2.1961,  2.9564, -4.1250, -0.3038,\n",
      "         -0.6767,  0.9855,  0.6946,  1.9225,  2.7871, -0.4976, -3.6438, -0.9251,\n",
      "         -0.2766, -0.9378, -0.1595, -4.5991,  3.4256,  3.1258, -0.2855,  0.8911,\n",
      "         -3.2432, -3.6302,  1.0653, -1.7685,  2.7951, -0.5020,  5.8457,  0.8831,\n",
      "          2.1439,  4.6838, -0.9085,  3.4227,  1.7218,  1.1241,  2.4837,  0.6861,\n",
      "          1.5328,  2.2228,  1.0220,  3.4905,  4.2310, -0.4667, -1.7831,  1.5569,\n",
      "         -2.5985,  3.6887,  0.4147,  2.5374,  2.5655,  2.0821, -0.4774,  0.7536,\n",
      "         -1.1623, -1.8772,  3.6906,  0.6255,  2.3397, -5.8938,  2.9370,  5.3190,\n",
      "          1.9937, -3.7964,  4.8428, -0.7071, -1.2060, -1.6025, -3.2592, -1.4988,\n",
      "         -5.2619,  2.3762, -2.4489, -3.0098,  0.7074, -1.9216,  1.4166, -2.8985,\n",
      "          3.7097,  1.1753,  4.4232,  0.2967, -0.5139,  3.4548,  0.1735, -0.7852,\n",
      "          0.2984,  1.9445,  1.9185, -3.6548,  5.6466, -4.7061,  2.2136, -2.7019,\n",
      "          2.7629, -3.8495,  1.2446, -0.8594,  1.7412,  0.6328,  0.5439,  1.3976,\n",
      "          2.0796, -1.8194,  2.3074, -0.3171,  1.0294, -2.6961,  3.0322, -0.4151,\n",
      "         -3.3846, -1.2357, -2.8764,  2.8143, -0.8950,  3.8401, -1.7580,  2.9597,\n",
      "         -0.0516, -1.2530, -1.1631,  1.3121,  2.8016,  1.5509, -0.0966, -1.3196,\n",
      "         -3.8512, -0.3926, -1.0125, -0.4627,  3.6530, -0.9110,  1.6485,  0.8766,\n",
      "         -0.9599,  5.3071,  2.5521, -4.1218,  6.2111, -3.5635,  7.2715, -1.1983,\n",
      "         -2.8805,  0.2405,  2.5419,  6.3268,  2.2479, -2.3478,  0.8101, -0.1191,\n",
      "          2.2647, -1.3514, -0.3039,  0.1084, -1.0040,  0.1856,  5.1085, -0.2239,\n",
      "         -4.0224,  1.3075, -2.2875,  6.6044, -3.3877,  0.4991, -1.2018, -4.3425,\n",
      "          0.9365,  0.3828, -1.8422,  0.6176,  0.8329,  0.9096, -2.6620, -5.1715,\n",
      "          2.2351,  0.7088,  6.7583,  0.7320, -0.1616,  0.6964,  0.4778,  1.2255,\n",
      "         -5.5209, -0.7538, -0.6154,  0.3482, -3.7442, -7.6140, -2.7207, -0.0710,\n",
      "          2.8965, -0.4667, -1.8133, -0.8989, -2.2897, -3.2142,  0.2378, -0.1977,\n",
      "         -0.7490,  3.3270, -5.5941,  0.3529, -0.7361,  2.7975, -1.6424,  2.8560]],\n",
      "       device='cuda:0'))\n",
      "1\n",
      "2\n",
      "1\n",
      "256\n",
      "384\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(features[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(features[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mtensor(features[\u001b[39m0\u001b[39;49m], device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m scans\u001b[39m.\u001b[39mappend(features)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "z = 0\n",
    "f = 0\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    t = t.cuda()\n",
    "\n",
    "    batch_features = []\n",
    "    for batch_scans in i:\n",
    "        scans = []\n",
    "        for scan in batch_scans:\n",
    "            if not is_zero_matrix(scan):\n",
    "                features = feature_model(scan.unsqueeze(0)) \n",
    "                print(features[0])\n",
    "                print(len(features))\n",
    "                print(len(features[0]))\n",
    "                print(len(features[0][0]))\n",
    "                print(len(features[0][0][0]))\n",
    "                print(len(features[0][0][0][0]))\n",
    "                print(torch.tensor(features[0], device='cuda'))\n",
    "                break\n",
    "                scans.append(features)\n",
    "        batch_features.append(scans)\n",
    "    break\n",
    "    outputs = [lc(batch_feature) for batch_feature in batch_features]\n",
    "    outputs = torch.stack(outputs, dim=0).squeeze(1)\n",
    "    print(outputs)\n",
    "    break\n",
    "    if z == 5:\n",
    "        break\n",
    "    z+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 256, 384] at entry 0 and [1, 384] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# torch.tensor(features[0])\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/exprements.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39;49mstack(features[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 256, 384] at entry 0 and [1, 384] at entry 1"
     ]
    }
   ],
   "source": [
    "# torch.tensor(features[0])\n",
    "torch.stack(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = make_segmentation_transform(resize_size=224)\n",
    "# target_transform = make_segmentation_target_transform(resize_size=224)\n",
    "transform = target_transform = make_classification_train_transform()\n",
    "\n",
    "train_dataset = make_dataset(\n",
    "    dataset_str=args.train_dataset_str,\n",
    "    transform=transform,\n",
    ")\n",
    "val_dataset = make_dataset(\n",
    "    dataset_str=args.val_dataset_str,\n",
    "    transform=transform,\n",
    ")\n",
    "test_dataset = make_dataset(\n",
    "    dataset_str=args.test_dataset_str,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "sampler_type = SamplerType.INFINITE\n",
    "\n",
    "train_data_loader = make_data_loader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=sampler_type,\n",
    "    sampler_advance=1,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "val_data_loader = make_data_loader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    sampler_type=sampler_type,\n",
    "    sampler_advance=1,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
