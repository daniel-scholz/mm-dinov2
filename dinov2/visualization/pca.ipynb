{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dinov2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/visualization/pca.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/dinov2/visualization/pca.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m (MultilabelAUROC, MultilabelF1Score, MultilabelAccuracy, MulticlassF1Score, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/dinov2/visualization/pca.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m                                         MulticlassAccuracy, MulticlassAUROC, Accuracy, BinaryF1Score, BinaryAUROC,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/dinov2/visualization/pca.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m                                         JaccardIndex, MulticlassJaccardIndex, Dice, BinaryAUROC)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/dinov2/visualization/pca.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfvcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m Checkpointer, PeriodicCheckpointer\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/dinov2/visualization/pca.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdinov2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdistributed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/dinov2/visualization/pca.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdinov2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munet\u001b[39;00m \u001b[39mimport\u001b[39;00m UNet\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/dinov2/visualization/pca.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdinov2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m SamplerType, make_data_loader, make_dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dinov2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "from typing import Any, Dict, Optional\n",
    "from collections import OrderedDict\n",
    "from monai.losses.dice import DiceLoss, DiceCELoss\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.wrappers import ClasswiseWrapper\n",
    "from torchmetrics.classification import (MultilabelAUROC, MultilabelF1Score, MultilabelAccuracy, MulticlassF1Score, \n",
    "                                        MulticlassAccuracy, MulticlassAUROC, Accuracy, BinaryF1Score, BinaryAUROC,\n",
    "                                        JaccardIndex, MulticlassJaccardIndex, Dice, BinaryAUROC)\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT, BTCV, BTCVSlice, AMOS, MSDHeart\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_padded_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets, make_data_loaders)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics, MetricAveraging, MetricType\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1024, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "patch_h = 40\n",
    "patch_w = 40\n",
    "# feat_dim = 384 # vits14\n",
    "# feat_dim = 768 # vitb14\n",
    "feat_dim = 1024 # vitl14\n",
    "# feat_dim = 1536 # vitg14\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.GaussianBlur(9, sigma=(0.1, 2.0)),\n",
    "    T.Resize((patch_h * 14, patch_w * 14)),\n",
    "    T.CenterCrop((patch_h * 14, patch_w * 14)),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "# dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14').cuda()\n",
    "# dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# extract features\n",
    "features = torch.zeros(4, patch_h * patch_w, feat_dim)\n",
    "imgs_tensor = torch.zeros(4, 3, patch_h * 14, patch_w * 14)\n",
    "for i in range(1, 5):\n",
    "    img_path = f'nih_images/img_{i+1}.png'\n",
    "    img = Image.open(img_path)\n",
    "    img = torch.from_numpy(np.stack((img,)*3, axis=-1))\n",
    "    print(img.shape)\n",
    "    imgs_tensor[i] = transform(img)[:3]\n",
    "with torch.no_grad():\n",
    "    features_dict = dinov2_vitl14.forward_features(imgs_tensor)\n",
    "    features = features_dict['x_norm_patchtokens']\n",
    "\n",
    "# PCA for feature inferred\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features = features.reshape(4 * patch_h * patch_w, feat_dim)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(features)\n",
    "pca_features = pca.transform(features)\n",
    "\n",
    "# visualize PCA components for finding a proper threshold\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(pca_features[:, 0])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(pca_features[:, 1])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(pca_features[:, 2])\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# uncomment below to plot the first pca component\n",
    "# pca_features[:, 0] = (pca_features[:, 0] - pca_features[:, 0].min()) / (pca_features[:, 0].max() - pca_features[:, 0].min())\n",
    "# for i in range(4):\n",
    "#     plt.subplot(2, 2, i+1)\n",
    "#     plt.imshow(pca_features[i * patch_h * patch_w: (i+1) * patch_h * patch_w, 0].reshape(patch_h, patch_w))\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# segment using the first component\n",
    "pca_features_bg = pca_features[:, 0] < 10\n",
    "pca_features_fg = ~pca_features_bg\n",
    "\n",
    "# plot the pca_features_bg\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(pca_features_bg[i * patch_h * patch_w: (i+1) * patch_h * patch_w].reshape(patch_h, patch_w))\n",
    "plt.show()\n",
    "\n",
    "# PCA for only foreground patches\n",
    "pca.fit(features[pca_features_fg]) # NOTE: I forgot to add it in my original answer\n",
    "pca_features_rem = pca.transform(features[pca_features_fg])\n",
    "for i in range(3):\n",
    "    # pca_features_rem[:, i] = (pca_features_rem[:, i] - pca_features_rem[:, i].min()) / (pca_features_rem[:, i].max() - pca_features_rem[:, i].min())\n",
    "    # transform using mean and std, I personally found this transformation gives a better visualization\n",
    "    pca_features_rem[:, i] = (pca_features_rem[:, i] - pca_features_rem[:, i].mean()) / (pca_features_rem[:, i].std() ** 2) + 0.5\n",
    "\n",
    "pca_features_rgb = pca_features.copy()\n",
    "pca_features_rgb[pca_features_bg] = 0\n",
    "pca_features_rgb[pca_features_fg] = pca_features_rem\n",
    "\n",
    "pca_features_rgb = pca_features_rgb.reshape(4, patch_h, patch_w, 3)\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(pca_features_rgb[i][..., ::-1])\n",
    "plt.savefig('features.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
