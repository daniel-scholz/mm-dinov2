{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import IPython \n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.eval.metrics import MetricAveraging, build_metric, build_segmentation_metrics\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_padded_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = argparse.AMOSace(backbone=\"dinov2\", config_file='dinov2/configs/eval/vitl14_pretrain.yaml', pretrained_weights='/home/baharoon/models/dinov2_vitl14_pretrain.pth', output_dir='results/test', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/home/baharoon/Data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/home/baharoon/Data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/home/baharoon/Data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "args = argparse.Namespace(backbone=\"dinov2\", config_file='dinov2/configs/eval/vitl14_pretrain.yaml', pretrained_weights='/home/baharoon/models/dinov2_vitl14_pretrain.pth', output_dir='results/test', opts=[], train_dataset_str='MC:split=TRAIN:root=/home/baharoon/Data/MC', val_dataset_str='MC:split=VAL:root=/home/baharoon/Data/MC', test_dataset_str='MC:split=TEST:root=/home/baharoon/Data/MC', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms()\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms()\n",
    "\n",
    "resize = transforms.Resize((448, 448), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "test_transformed = make_dataset(\n",
    "    dataset_str=args.test_dataset_str,\n",
    "    transform=eval_image_transform,\n",
    "    target_transform=eval_target_transform,\n",
    ")\n",
    "\n",
    "test = make_dataset(\n",
    "    dataset_str=args.test_dataset_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01]\n",
    "num_of_classes = test.get_num_classes()\n",
    "is_3d = test.is_3d()\n",
    "embed_dim = model.embed_dim\n",
    "decoder_type = \"linear\"\n",
    "resize_size = 448\n",
    "decoders, optim_param_groups = setup_decoders(\n",
    "    embed_dim,\n",
    "    learning_rates,\n",
    "    num_of_classes,\n",
    "    decoder_type,\n",
    "    is_3d=is_3d,\n",
    "    image_size=resize_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/baharoon/dinov2/results/mcdinov2vitllinear/optimal/model_final.pth\"\n",
    "checkpointer = Checkpointer(decoders, output_dir)\n",
    "start_iter = checkpointer.resume_or_load(output_dir, resume=True).get(\"iteration\", -1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "decoder = list(decoders.module.decoders_dict.values())[0]\n",
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx)\n",
    "output_dir = \"/home/baharoon/dinov2/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_multipler = 255//num_of_classes\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.SEGMENTATION_METRICS, num_labels=num_of_classes).cuda()\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/freefont/FreeMono.ttf\", 15)\n",
    "num_of_images = 1\n",
    "for image_index in range(num_of_images):\n",
    "\n",
    "    image, target = test_transformed[image_index]\n",
    "    image, target = image.cuda(non_blocking=True).unsqueeze(0), target.cuda(non_blocking=True).unsqueeze(0)\n",
    "\n",
    "    untransformed_image = resize(test[image_index][0])[0].cuda()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        features = feature_model(image)\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=resize_size, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    results = metric(prediction, target)\n",
    "\n",
    "    prediction = prediction.squeeze()\n",
    "    prediction = (prediction * highlight_multipler).cpu()\n",
    "    H, W = prediction.squeeze().shape\n",
    "    pil_image = torchvision.transforms.ToPILImage()(prediction.type(torch.int32))\n",
    "    pil_image = pil_image.convert(\"L\") # Convert to Grayscale\n",
    "    \n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    result_meta = \"\"\n",
    "    for m, r in dict(results).items():\n",
    "        result_meta += f\"{m}: {float(r):.3f} \"\n",
    "\n",
    "    draw.text((0, 0), result_meta, fill=255)\n",
    "\n",
    "    pil_image.save(f\"{output_dir}/{test_transformed.images[image_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
