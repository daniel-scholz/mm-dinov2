{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import IPython \n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import make_segmentation_target_transform, make_segmentation_transform\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features\n",
    "from dinov2.eval.metrics import build_segmentation_metrics, MetricAveraging\n",
    "from dinov2.utils import show_image_from_tensor\n",
    "from dinov2.eval.segmentation import setup_decoders, TransformerEncoder, LinearDecoder\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230829 01:18:33 23211 dinov2 config.py:60] git:\n",
      "  sha: eff0d2f88dd87cac50bf6981d4488304dacd7254, status: has uncommitted changes, branch: main\n",
      "\n",
      "I20230829 01:18:33 23211 dinov2 config.py:61] batch_size: 8\n",
      "comment: \n",
      "config_file: dinov2/configs/eval/vits14_pretrain.yaml\n",
      "exclude: \n",
      "gather_on_cpu: False\n",
      "n_per_class_list: [-1]\n",
      "n_tries: 1\n",
      "nb_knn: [5, 20, 50, 100, 200]\n",
      "ngpus: 1\n",
      "nodes: 1\n",
      "opts: ['train.output_dir=/mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn']\n",
      "output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "partition: learnlab\n",
      "pretrained_weights: models/dinov2_vits14_pretrain.pth\n",
      "temperature: 0.07\n",
      "timeout: 2800\n",
      "train_dataset_str: MC:split=TRAIN:root=/mnt/z/data/MC/train\n",
      "use_volta32: False\n",
      "val_dataset_str: MC:split=VAL:root=/mnt/z/data/MC/test\n",
      "I20230829 01:18:33 23211 dinov2 config.py:27] sqrt scaling learning rate; base: 0.004, new: 0.001\n",
      "I20230829 01:18:33 23211 dinov2 config.py:34] MODEL:\n",
      "  WEIGHTS: ''\n",
      "compute_precision:\n",
      "  grad_scaler: true\n",
      "  teacher:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "  student:\n",
      "    backbone:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp16\n",
      "        buffer_dtype: fp32\n",
      "    dino_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "    ibot_head:\n",
      "      sharding_strategy: SHARD_GRAD_OP\n",
      "      mixed_precision:\n",
      "        param_dtype: fp16\n",
      "        reduce_dtype: fp32\n",
      "        buffer_dtype: fp32\n",
      "dino:\n",
      "  loss_weight: 1.0\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "  koleo_loss_weight: 0.1\n",
      "ibot:\n",
      "  loss_weight: 1.0\n",
      "  mask_sample_probability: 0.5\n",
      "  mask_ratio_min_max:\n",
      "  - 0.1\n",
      "  - 0.5\n",
      "  separate_head: false\n",
      "  head_n_prototypes: 65536\n",
      "  head_bottleneck_dim: 256\n",
      "  head_nlayers: 3\n",
      "  head_hidden_dim: 2048\n",
      "train:\n",
      "  batch_size_per_gpu: 64\n",
      "  dataset_path: ImageNet:split=TRAIN\n",
      "  output_dir: /mnt/c/Users/user/Desktop/dinov2/results/NIH/dinov2_vits14/knn\n",
      "  saveckp_freq: 20\n",
      "  seed: 0\n",
      "  num_workers: 10\n",
      "  OFFICIAL_EPOCH_LENGTH: 1250\n",
      "  cache_dataset: true\n",
      "  centering: centering\n",
      "student:\n",
      "  arch: vit_small\n",
      "  patch_size: 14\n",
      "  drop_path_rate: 0.3\n",
      "  layerscale: 1.0e-05\n",
      "  drop_path_uniform: true\n",
      "  pretrained_weights: ''\n",
      "  ffn_layer: mlp\n",
      "  block_chunks: 0\n",
      "  qkv_bias: true\n",
      "  proj_bias: true\n",
      "  ffn_bias: true\n",
      "teacher:\n",
      "  momentum_teacher: 0.992\n",
      "  final_momentum_teacher: 1\n",
      "  warmup_teacher_temp: 0.04\n",
      "  teacher_temp: 0.07\n",
      "  warmup_teacher_temp_epochs: 30\n",
      "optim:\n",
      "  epochs: 100\n",
      "  weight_decay: 0.04\n",
      "  weight_decay_end: 0.4\n",
      "  base_lr: 0.004\n",
      "  lr: 0.001\n",
      "  warmup_epochs: 10\n",
      "  min_lr: 1.0e-06\n",
      "  clip_grad: 3.0\n",
      "  freeze_last_layer_epochs: 1\n",
      "  scaling_rule: sqrt_wrt_1024\n",
      "  patch_embed_lr_mult: 0.2\n",
      "  layerwise_decay: 0.9\n",
      "  adamw_beta1: 0.9\n",
      "  adamw_beta2: 0.999\n",
      "crops:\n",
      "  global_crops_scale:\n",
      "  - 0.32\n",
      "  - 1.0\n",
      "  local_crops_number: 8\n",
      "  local_crops_scale:\n",
      "  - 0.05\n",
      "  - 0.32\n",
      "  global_crops_size: 518\n",
      "  local_crops_size: 98\n",
      "evaluation:\n",
      "  eval_period_iterations: 12500\n",
      "\n",
      "I20230829 01:18:33 23211 dinov2 vision_transformer.py:110] using MLP layer as FFN\n",
      "I20230829 01:18:34 23211 dinov2 utils.py:35] Pretrained weights found at models/dinov2_vits14_pretrain.pth and loaded with msg: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='MC:split=TRAIN:root=/mnt/z/data/MC/train', val_dataset_str='MC:split=VAL:root=/mnt/z/data/MC/test', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230829 01:18:34 23211 dinov2 loaders.py:90] using dataset: \"MC:split=TRAIN:root=/mnt/z/data/MC/train\"\n",
      "0 scans are missing from TRAIN set\n",
      "I20230829 01:18:34 23211 dinov2 loaders.py:95] # of dataset samples: 92\n",
      "I20230829 01:18:34 23211 dinov2 loaders.py:90] using dataset: \"MC:split=VAL:root=/mnt/z/data/MC/test\"\n",
      "46 scans are missing from VAL set\n",
      "I20230829 01:18:34 23211 dinov2 loaders.py:95] # of dataset samples: 46\n"
     ]
    }
   ],
   "source": [
    "transform = make_segmentation_transform(resize_size=448)\n",
    "target_transform = make_segmentation_target_transform(resize_size=448)\n",
    "\n",
    "train_dataset = make_dataset(\n",
    "    dataset_str=args.train_dataset_str,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "val_dataset = make_dataset(\n",
    "    dataset_str=args.val_dataset_str,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "sampler_type = SamplerType.INFINITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.embed_dim\n",
    "decoders, optim_param_groups = setup_decoders(\n",
    "    embed_dim,\n",
    "    [1e-6, 2e-6, 5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 5e-2, 1e-1],\n",
    "    3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:150] [Checkpointer] Loading from models/trained_heads/segmentation_linear/model_final.pth ...\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0000010000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0000020000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0000050000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0000100000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0000200000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0000500000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0001000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0002000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0005000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0010000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0020000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0050000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0100000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_0500000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:338] Skip loading parameter 'decoders_dict.segmentor_lr_0_1000000000.decoder.weight' to the model due to incompatible shapes: (3, 1024, 1, 1) in the checkpoint but (3, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
      "W20230829 01:18:34 23211 fvcore.common.checkpoint checkpoint.py:350] Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0000010000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0000020000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0000050000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0000100000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0000200000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0000500000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0001000000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0002000000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0005000000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0010000000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0020000000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0050000000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0100000000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_0500000000.decoder.weight\u001b[0m\n",
      "\u001b[34mdecoders_dict.segmentor_lr_0_1000000000.decoder.weight\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"models/trained_heads/segmentation_linear/model_final.pth\"\n",
    "checkpointer = Checkpointer(decoders, output_dir)\n",
    "start_iter = checkpointer.resume_or_load(output_dir, resume=True).get(\"iteration\", -1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "decoder = decoders.module.decoders_dict[\"segmentor_lr_0_1000000000\"]\n",
    "feature_model = TransformerEncoder(model, autocast_ctx=autocast_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dice': tensor(0., device='cuda:0'), 'jaccard': tensor(0., device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "highlight_multipler = 50\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.SEGMENTATION_METRICS, num_labels=3).cuda()\n",
    "result_message = \"\"\n",
    "for image, target in val_dataset:\n",
    "    image, target = image.cuda(non_blocking=True).unsqueeze(0), target.cuda(non_blocking=True).unsqueeze(0)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    results = metric(prediction, target)\n",
    "\n",
    "    target = target.squeeze()\n",
    "    prediction = prediction.squeeze()\n",
    "\n",
    "    concated = torch.cat((target, prediction), dim=-1)\n",
    "\n",
    "    prediction = (prediction * highlight_multipler).cpu()\n",
    "    target = (target * highlight_multipler).cpu()\n",
    "    concated = (concated.unsqueeze(0).type(torch.int32) * highlight_multipler).cpu()\n",
    "    H, W = concated.squeeze().shape\n",
    "\n",
    "    pil_image = torchvision.transforms.ToPILImage()(concated)\n",
    "    pil_image = pil_image.convert(\"L\") # Convert to Grayscale\n",
    "\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    _, _, w, h = draw.textbbox((0, 0), \"Target\")\n",
    "    draw.text(((W-w)*0.25, H*0.025), \"Target\", fill=255)\n",
    "\n",
    "    _, _, w, h = draw.textbbox((0, 0), \"Prediction\")\n",
    "    draw.text(( (W-w) * 0.75, H*0.025), \"Prediction\", fill=255)\n",
    "\n",
    "    for m, r in dict(results).items():\n",
    "        result_message += f\"{m}: {float(r):.3f} \"\n",
    "\n",
    "    draw.text((0, 0), result_message, fill=255)\n",
    "\n",
    "    pil_image.save(\"hello.png\")\n",
    "    print(metric.compute())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
